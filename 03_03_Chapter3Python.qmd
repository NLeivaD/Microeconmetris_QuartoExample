---
title: "Chapter 3"
bibliography: refs.bib
---

# Code Call Outs

## Code Call Out 3.1 - Propensity Score Matching and Job Training Programs

In observational studies, where treatment assignment is not random, estimating causal effects can be challenging due to potential confounding factors. One method to address this challenge is Propensity Score Matching (PSM) which assumes *selection on observables*. PSM aims to control for observed confounding by matching treated units with untreated units that have similar propensity scores. The propensity score for a unit is the probability of receiving the treatment given observed covariates. By matching on propensity scores, we aim to create a scenario where the distribution of observed covariates is similar between the treated and untreated groups, mimicking a randomized experiment. This method allows to estimate causal treatment effects in observational settings, making it a valuable tool in microeconometrics.

In this example, we will consider a setting studied by @Lalonde1986 and @DeheijaWahba2002, @DeheijaWahba1999.  There, the authors sought to compare an estimate based on observational methods with an experimental programme evaluation, to see how well observational methods compare to experimental estimates. The original experimental estimates suggested that the receipt of a job training program increased earnings by $1,794 using this estimation sample.   In this code call out, we work with data provided by @DeheijaWahba1999 which consists of an experimentally treated samples, as well as a similar sample of untreated units drawn from a large survey database (the CPS and PSID).  The dataset contains information on individuals' participation in the program (treat), their earnings in 1978 (re78) which follows program participation (in the case of treated observations), and several other covariates such as age, education, race, marital status, and earnings in 1974 and 1975 (pre-treatment outcomes). Our main objective in this exercise is to estimate the Average Treatment Effect on the Treated (ATT) of the job training program on earnings in 1978 using Propensity Score Matching.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import seaborn as sns
sns.set()
sns.set_palette("viridis", n_colors=2)

# Load the dataset
###TENEMOS QUE LIMPIAR ESTO -- ES SOLO PARA DEFINIR LOS MISMOS CONTROLES QUE ELLOS
data = pd.read_stata('Datasets/Dehejia_Wahba_1999.dta')
##AQUÍ CONFIRMAMOS EL VALOR EXPERIMENTAL (FILA 1 DE TABLA 2)
Y1 = data.loc[data['treat'] == 1, 're78'].mean()
Y0 = data.loc[data['treat'] == 0, 're78'].mean()
print(Y1-Y0)

data2 = pd.read_stata('Datasets/cps_controls.dta')
d1treat = data[data['treat']==1]
data    = pd.concat([d1treat,data2])
data['age2']=data['age']*data['age']
data['age3']=data['age2']*data['age']
data['school2']=data['education']*data['education']
data['u74']=data['re74']==0
data['u75']=data['re75']==0
data['schoolXre74']=data['education']*data['re74']

# Define the covariates and the treatment variable
X = data[['age','age2','age3','education','school2','married','nodegree',
          'black', 'hispanic', 're74','re75','u74','u75','schoolXre74']]
y = data['treat']

print(sum(data['treat']))

### ACTUALMENTE LOGIT NO CONVERGE...  SOSPECHO QUE ESTO HACE QUE LAS RESPUESTAS SEAN DISTINTAS...
# Estimate propensity scores using logistic regression
logit = LogisticRegression(max_iter=1000000)
logit.fit(X, y)
data['propensity_score'] = logit.predict_proba(X)[:, 1]

# Perform matching
treated = data[data.treat == 1]
untreated = data[data.treat == 0]
neigh = NearestNeighbors(n_neighbors=1)
neigh.fit(untreated[['propensity_score']])
indices = neigh.kneighbors(treated[['propensity_score']], return_distance=False)
matched = untreated.iloc[indices.flatten()]
#### DEBEN SER 119 DISTINTOS SEGÚN FILA 6 DE TABLA 2...
print(matched['propensity_score'].nunique())

# Reset indexes for treated and matched DataFrames
treated = treated.reset_index(drop=True)
matched = matched.reset_index(drop=True)

# Diagnostic Checks
# 1. Check for Missing Values
print(data[['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're74', 're75', 're78']].isnull().sum())

# 2. Check Matching Results
print("Number of treated individuals:", len(treated))
print("Number of matched untreated individuals:", len(matched))

# 3. Check Propensity Scores
print("Unique propensity scores:", data['propensity_score'].nunique())

# Calculate ATT
ATT = (treated['re78'] - matched['re78']).mean()
print(f"Average Treatment Effect on the Treated (ATT): {ATT}")
```

```{python}
# Plot the histograms with outlined bars and a chosen fill color
plt.hist(treated['propensity_score'], bins=50, linewidth=1.2,label='Treated (NSW)', alpha=0.7)
plt.hist(untreated['propensity_score'], bins=50, linewidth=1.2, label='Control (CPS or PSID)', alpha=0.7)
plt.legend(loc='best')
plt.xlabel('Propensity Score')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Boxplot for earnings in 1978 (re78) distribution comparison
sns.boxplot(x='treat', y='re78', data=pd.concat([treated, matched]))
plt.xlabel('Treatment')
plt.ylabel('Earnings in 1978 (re78)')
plt.title('Boxplot for Earnings in 1978 Distribution Comparison')
plt.xticks([0, 1], ['Untreated', 'Treated'])
plt.show()


```
## Code Call Out 3.2 - Considering Overlap and Variable Balance
Maternal smoking during pregnancy has been a subject of extensive study due to its potential impact on infant health outcomes, such as birth weight. However, simply comparing the birth weights of infants born to smokers versus non-smokers may not account for confounding factors that influence both the likelihood of smoking and birth outcomes.  In this code call out, we will work with data from @Almondetal2005 which consists of a child's birthweight, an indicator of whether their mother smoked during pregnancy, and a number of covariates.

```{python}

import pandas as pd
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns

```
```{python}
# Read in data
birth_weight = pd.read_csv("Datasets/Birth_Weight.csv")

# Recod variables in binary
birth_weight['mmarried'] = birth_weight['mmarried'].apply(lambda x: 1 if x == "Married" else 0)
birth_weight['fbaby'] = birth_weight['fbaby'].apply(lambda x: 1 if x == "Yes" else 0)
birth_weight['mbsmoke'] = birth_weight['mbsmoke'].apply(lambda x: 1 if x == "Smoker" else 0)

# Consider an OLS regression conditioning on covariates
ols_model = smf.ols('bweight ~ mbsmoke + mmarried + mage + medu + fbaby', data=birth_weight).fit()
print(ols_model.summary())

# Estimate propensity score
X = birth_weight[['mmarried', 'mage', 'medu', 'fbaby']]
y = birth_weight['mbsmoke']
logit = LogisticRegression()
logit.fit(X, y)
birth_weight['pscore'] = logit.predict_proba(X)[:, 1]

# Examine overlap
sns.kdeplot(birth_weight[birth_weight['mbsmoke'] == 0]['pscore'], shade=True, label='No Smoker')
sns.kdeplot(birth_weight[birth_weight['mbsmoke'] == 1]['pscore'], shade=True, label='Smoker')
plt.xlabel('Propensity Score')
plt.ylabel('Density')
plt.show()

# Conduct nearest-neighbour matching
smokers = birth_weight[birth_weight['mbsmoke'] == 1]
non_smokers = birth_weight[birth_weight['mbsmoke'] == 0]
nn = NearestNeighbors(n_neighbors=1)
nn.fit(non_smokers[['pscore']])
matches = nn.kneighbors(smokers[['pscore']], return_distance=False)
matched_smokers = smokers
matched_non_smokers = non_smokers.iloc[matches[:, 0]]

# Comare matched groups
matched_data = pd.concat([matched_smokers, matched_non_smokers])
matched_data.boxplot(column='bweight', by='mbsmoke')
plt.xlabel('Mother Smoked')
plt.ylabel('Birth Weight')
plt.show()

```

## Code Call Out 3.3 - Inverse Propensity Score Weighting 
```{python}
# Import Libraries and Load Data
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
import statsmodels.api as sm
import statsmodels.formula.api as smf
from dowhy import CausalModel as DoWhyCausalModel
from sklearn.neighbors import NearestNeighbors

# Load the data
data = pd.read_stata('Datasets/Dehejia_Wahba_1999.dta')

# Define covariates and treatment variable
covariates = ['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're74', 're75']
treatment = 'treat'
outcome = 're78'



# DoWhy CausalModel for Propensity Score Methods
dowhy_model = DoWhyCausalModel(
    data=data,
    treatment=treatment,
    outcome=outcome,
    common_causes=covariates
)
identified_estimand = dowhy_model.identify_effect(proceed_when_unidentifiable=True)
ps_methods = ["matching", "weighting"]
ate_dowhy = {}
att_dowhy = {}
for method in ps_methods:
    estimate_ate = dowhy_model.estimate_effect(
        identified_estimand,
        method_name=f"backdoor.propensity_score_{method}",
        target_units="ate"
    )
    estimate_att = dowhy_model.estimate_effect(
        identified_estimand,
        method_name=f"backdoor.propensity_score_{method}",
        target_units="att"
    )
    ate_dowhy[method] = estimate_ate.value
    att_dowhy[method] = estimate_att.value

# Estimating Propensity Scores
logit_model = LogisticRegression()
X = data[covariates]
y = data[treatment]
logit_model.fit(X, y)
data['pscore'] = logit_model.predict_proba(X)[:, 1]

# Propensity Score Matching for ATT
treated = data[data[treatment] == 1]
control = data[data[treatment] == 0]
nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(control[covariates])
distances, indices = nn.kneighbors(treated[covariates])
matched_control = control.iloc[indices.flatten()]
matched_data = pd.concat([treated, matched_control])

# Regression for ATT on Matched Data
X_matched = sm.add_constant(matched_data[covariates + [treatment]])
y_matched = matched_data[outcome]
model_matched = sm.OLS(y_matched, X_matched).fit()
att_matched = model_matched.params[treatment]
# Linear Regression for ATE
lm_ate = LinearRegression().fit(data[[treatment]], data[outcome])
ate_regression = lm_ate.coef_[0]

# Display Results
print(f"ATE by Regression: {ate_regression}")
print(f"ATT by Matching and Regression: {att_matched}")
for method, estimate in ate_dowhy.items():
    print(f"ATE by {method.capitalize()}: {estimate}")
for method, estimate in att_dowhy.items():
    print(f"ATT by {method.capitalize()}: {estimate}")


```
