---
title: "Chapter 4"
bibliography: refs.bib
---

```{r echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, eval = T, message = F)
library(Statamarkdown)
```

# Code Call-outs

## Code Call-out 4.1: Wild cluster bootstrap implementation
To see the difference between the wild cluster bootstrap described in Section 4.2.3.2 of the book and other clustering options such as standard cluster bootstrap or clustered standard errors we will set up an example by hand of the wild cluster bootstrap.  We will do this with data provided by @PorterSerra2020 who conducted a field experiment which sought to test whether student exposure to engaging and successful women instructors in early economics classes increases the likelihood that female students go on to major in economics.  The dataset is provided under the name `PorterSerra2020.csv`, and we will open it below:

```{stata}
import delimited "Datasets\PorterSerra2020.csv", clear
```

Following @PorterSerra2020 we will estimate the following linear probability model (LPM)
$$Y_{i} = \beta_0 + \beta_1 dt_i + \beta_2 dT_i + \beta_3 dt_i \times dT_i + \delta \mathbf{X}_i + \varepsilon_i$$
where we use identical notation from their paper.  Treatment was randomly applied at the class level in 2016, and classes also existed in 2015, but no treatment was applied.  Above, $Y_i$ is a student's (binary) decision of whether or not to major in economics (`econmajor`), $dt_i$ (`yr_2016`) a dummy equal to one if she took the class in 2016 and zero if she took a class in 2015, and $dT_i$ (`treatment_class`) is a dummy equal to one if she is in a treatment class, and zero if she is in a control class. The interaction between these two dummies (`treat2016`) is the coefficient of interest and $\mathbf{X}_i$ is a vector of individual, demographic and class controls such as if the course was taught by a female professor (`female_prof`), if the student is an in-state student (`instate`), if the student is in freshman year (`freshman`), if the student is american (`american`), the student's cumulative GPA (`acumgpa`), the student's grade in their Principles of Economics course (`gradeprinciples`) and if the student take a class with a limit of 40 students (`small_class`).  As treatment is assigned at the class level (`class_fe2`), and as there are few clusters (12 clusters), the authors proceed to conduct inference using a wild cluster bootstrap.  We conduct this procedure below.

Here in particular we are interested in the parameter $\beta_3$ which under difference-in-difference assumptions will identify the effect of female role models on future enrollment in an economics major.  Before examining this process, we will estimate the LPM in order to get an estimate of the coefficient of interest $\widehat{\beta}_3$, along with the (traditional) cluster-robust standard error $se\left(\widehat{\beta}_3\right)$, and resulting $t$-statistic for the test of a null effect: $t=\left(\widehat{\beta}_3 - 0\right)/se\left(\widehat{\beta}_3\right)$:

```{stata echo = 2:3}
qui import delimited "Datasets\PorterSerra2020.csv", clear
keep if female == 1
reg econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
```

It can be seen that the coefficient of interest is 0.0801 (as per column 4 of Table 4 of @PorterSerra2020) with a cluster-robust standard error of 0.0364 and a resulting t-statistic of 2.197. Below, we store these values along with the residuals of this unrestricte regression $\widehat{\varepsilon}$:

```{stata echo = 4:7}
qui import delimited "Datasets\PorterSerra2020.csv", clear
qui keep if female == 1
qui reg econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
scalar beta3_hat = _b["treat2016"]
scalar se_beta3_hat = _se["treat2016"]
scalar t_beta3_hat = _b["treat2016"] / _se["treat2016"]
predict eps_hat, resid
```

Because we are interested in considering the variation of data in a model where we assume the null hypothesis $\beta_3=0$ is true, we will now *impose* this hypothesis, and re-estimate our model.  We do this below, imposing the restriction $\beta_3 = 0$ by simply omiting the `treat2016` variable from the model, storing the restricted residuals from this regression as $\tilde{\varepsilon}$. 

```{stata echo = 3:5}
qui import delimited "Datasets\PorterSerra2020.csv", clear
qui keep if female == 1
reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
matrix LPM_r = e(b)
predict eps_tilde, resid
```

These restricted residuals `eps_tilde` above will be key in our wild cluster bootstrap procedure.  For a given bootstrap replication, for each cluster we will assign a value of -1 or +1, and multiply the previous residuals by this (cluster-specific) value.  This will maintain correlations between residuals fixed within each cluster, but allow correlations to vary between clusters.  We will thus generate a new "sample" of data taking original data and updated residuals, resulting in a new outcome for $Y_i$.

Below we will initialise this wild cluster bootstrap procedure, setting some large amount of bootstrap replicates in a global macro (here 999), before storing the data we need as `b_sample` using `Stata`'s new feature `frames` to store multiple datasets in one session.  We will then also incorporate the residuals from above into this dataframe, so `b_sample` contains all relevant covariates, as well as the restricted residuals.  It is worth noting, that in practice, all we require from these covariates is the ability to form $\widehat{Y}_i=\widehat\beta_0+\widehat\beta_1 dt_i + \widehat\beta_2 dT_i + \widehat\delta \mathbf{X}_i$, and we could actually just work with the quantity $\widehat{Y}_i$ below (you may wish to confirm this to yourself by editing the code below).  However, for ease of exposition we will work with the full set of covariates in code below, even though this is somewhat less efficient.

```{stata echo = 7:18}
quietly{
    import delimited "Datasets\PorterSerra2020.csv", clear
	keep if female == 1
	reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
	predict eps_tilde, resid
}
* Global macro for setting bootstrap amount of replications
global B = 999

* Matrix to store iterations results
matrix WildClusterBootstrap = J($B, 3, .)
matrix colnames WildClusterBootstrap = "beta3" "se_beta3" "t_stat"

* Keep relevant variables
keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde

* Store them in a data frame
frame put *, into(b_sample)
```

Now let's see what each iteration of a wild cluster bootstrap looks like.  As we will generate our new sample of data by (randomly) selecting values of -1 or 1 for each cluster to form "resampled" residuals, we will start by drawing these "Rademacher" weights for each cluster.  Below we do this by first generating a cluster-specific draw for each cluster $g$ which assigns $a_g = 1$ or $a_g = -1$ with probability 0.5 (as seen in `clusters`).  This value $a_g$ is joined into our main data:
```{stata echo = 9:31}
quietly{
    import delimited "Datasets\PorterSerra2020.csv", clear
	keep if female == 1
	reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
	predict eps_tilde, resid
	keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
	frame put *, into(b_sample)
}
* Keep cluster variable
keep class_fe2

* Keep unique values
duplicates drop

* Assign this values into a new dataframe
frame put *, into(clusters)

* Activate the clusters dataframe
frame change clusters

* Generate a random -1 or 1 for each cluster
gen ag = cond(runiform() < 0.5, -1, 1)

* Activate the bootstrap sample dataframe
frame change b_sample

* Link with the clusters dataframe
frlink m:1 class_fe2, frame(clusters)

* Join the cluster "Rademacher" weight information
frget ag, from(cluster)
```
Now, based on this draw and the original errors from the restricted model, we will generate the new set of bootstrap errors, which below we call `berrors`:
```{stata echo = 17}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  predict eps_tilde, resid
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
}
gen berrors = eps_tilde * ag
```
Finally, below we will generate our new resampled outcome variable `beconmajor` from covariates, restricted regression estimates, and our resampled error term `berrors`.
```{stata echo = 19}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
}
gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
```

With this data in hand, we estimate the non-restricted model exactly as we did so previously with `felm`.  Below, we estimate this model, and examine summary output:

```{stata echo = 20}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
}
reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
```

You will note here that the coefficient of interest (that on `treat2016`) is small and insignificant.  This should not be surprising to us, as we have imposed that this coefficient should be zero in the process where we generated `beconmajor` previously.  The idea of this process is that in this way we should have some idea of the variation we may expect in parameter estimates when the true parameter actually *is* zero.  If we observe that our true estimate greatly exceeds these "null" estimates, we may be willing to conclude that the original effect is real.  We store the relevant values from our regression model below to calculate a t-statistic from this bootstrap replicate.

```{stata echo = 23:25}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  global B = 999
  matrix WildClusterBootstrap = J($B, 3, .)
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
}
matrix define WildClusterBootstrap[1,1] = _b["treat2016"]
matrix define WildClusterBootstrap[1,2] = _se["treat2016"]
matrix define WildClusterBootstrap[1,3] = _b["treat2016"] / _se["treat2016"]
```
We wish to see how extreme our original t-statistic is compared to many t-statistics generated in this way, where the null is imposed.  Thus, we will now repeat the previous bootstrap replicate $B-1$ more times in a loop, so that we have $B$ t-statistics. Note we use `quietly` to ommit all intermediate output.

```{stata echo = 26:42}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  global B = 999
  matrix WildClusterBootstrap = J($B, 3, .)
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix define WildClusterBootstrap[1,1] = _b["treat2016"]
  matrix define WildClusterBootstrap[1,2] = _se["treat2016"]
  matrix define WildClusterBootstrap[1,3] = _b["treat2016"] / _se["treat2016"]
}
forvalues b = 2/$B{
    quietly{
		drop beconmajor berrors ag clusters
		frame change clusters
		drop ag
		gen ag = cond(runiform() < 0.5, -1, 1)
		frame change b_sample
		frlink m:1 class_fe2, frame(clusters)
		frget ag, from(clusters)
		gen berrors = eps_tilde * ag
		gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
		reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
		matrix define WildClusterBootstrap[`b',1] = _b["treat2016"]
		matrix define WildClusterBootstrap[`b',2] = _se["treat2016"]
		matrix define WildClusterBootstrap[`b',3] = _b["treat2016"] / _se["treat2016"]
	}
}
```

We can see below what this "null distribution" of t-statistics looks like.  It is not a surprise that these are centred around 0, because this is what our model has imposed.  However, more interesting than this is to see they type of variation in t-statistics which we can expect in our data with null effects imposed.  We can see, below, that this looks somewhat heavier-tailed than a standard t-distribution.

```{stata echo = 42:44}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  global B = 999
  matrix WildClusterBootstrap = J($B, 3, .)
  matrix colnames WildClusterBootstrap = "beta3" "se_beta3" "t_stat"
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix define WildClusterBootstrap[1,1] = _b["treat2016"]
  matrix define WildClusterBootstrap[1,2] = _se["treat2016"]
  matrix define WildClusterBootstrap[1,3] = _b["treat2016"] / _se["treat2016"]
  forvalues b = 2/$B{
	  drop beconmajor berrors ag clusters
	  frame change clusters
	  drop ag
	  gen ag = cond(runiform() < 0.5, -1, 1)
	  frame change b_sample
	  frlink m:1 class_fe2, frame(clusters)
	  frget ag, from(clusters)
	  gen berrors = eps_tilde * ag
	  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
	  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
	  matrix define WildClusterBootstrap[`b',1] = _b["treat2016"]
	  matrix define WildClusterBootstrap[`b',2] = _se["treat2016"]
	  matrix define WildClusterBootstrap[`b',3] = _b["treat2016"] / _se["treat2016"]
  }
}
clear
svmat WildClusterBootstrap, names(col)
histogram t_stat, bin(20) scheme(plottig) color(blue*2)
qui graph export "WCB_Histogram.png", replace
```

![Wild Cluster Bootstrap Histogram](WCB_Histogram.png)

From this distribution we can calculate a p-value by asking what proportion of t-statistics from the null distribution exceed our estimated t-statistic from the unrestricted model.  We do this below, observing that the p-value is quite close to that reported in @PorterSerra2020 (who report a p-value of 0.089), only differing due to random variation in draws of the Rademacher weights.


<!--
Now we compare our original $t$-statistic `r round(t_beta3_hat, 3)` with the 97.5 percentile of the $t$-statistics computed in our bootstraping to see if we can reject the null of $\beta_3 = 0$ to a 5% significance level

```{r}
t97_5 <- quantile(WildClusterBootstrap$t_stat, 0.975)
if(abs(t_beta3_hat) > abs(t97_5)){
  print("The null is rejected.")
} else {
  print("We can't reject the null.")
}
```
-->

```{stata echo = 46:49}
quietly{
  import delimited "Datasets\PorterSerra2020.csv", clear
  keep if female == 1
  reg econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  scalar t_beta3_hat = _b["treat2016"] / _se["treat2016"]
  reg econmajor yr_2016 treatment_class female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix LPM_r = e(b)
  predict eps_tilde, resid
  global B = 999
  matrix WildClusterBootstrap = J($B, 3, .)
  matrix colnames WildClusterBootstrap = "beta3" "se_beta3" "t_stat"
  keep econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class class_fe2 eps_tilde
  frame put *, into(b_sample)
  keep class_fe2
  duplicates drop
  frame put *, into(clusters)
  frame change clusters
  gen ag = cond(runiform() < 0.5, -1, 1)
  frame change b_sample
  frlink m:1 class_fe2, frame(clusters)
  frget ag, from(cluster)
  gen berrors = eps_tilde * ag
  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
  matrix define WildClusterBootstrap[1,1] = _b["treat2016"]
  matrix define WildClusterBootstrap[1,2] = _se["treat2016"]
  matrix define WildClusterBootstrap[1,3] = _b["treat2016"] / _se["treat2016"]
  forvalues b = 2/$B{
	  drop beconmajor berrors ag clusters
	  frame change clusters
	  drop ag
	  gen ag = cond(runiform() < 0.5, -1, 1)
	  frame change b_sample
	  frlink m:1 class_fe2, frame(clusters)
	  frget ag, from(clusters)
	  gen berrors = eps_tilde * ag
	  gen beconmajor = LPM_r[1, 10] + LPM_r[1, 1] * yr_2016 + LPM_r[1, 2] * treatment_class + LPM_r[1, 3] * female_prof + LPM_r[1, 4] * instate + LPM_r[1, 5] * freshman + LPM_r[1, 6] * american + LPM_r[1, 7] * acumgpa + LPM_r[1, 8] * gradeprinciples + LPM_r[1, 9] * small_class + berrors
	  reg beconmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
	  matrix define WildClusterBootstrap[`b',1] = _b["treat2016"]
	  matrix define WildClusterBootstrap[`b',2] = _se["treat2016"]
	  matrix define WildClusterBootstrap[`b',3] = _b["treat2016"] / _se["treat2016"]
  }
  clear
  svmat WildClusterBootstrap, names(col)
}
gen aux = abs(t_stat) > abs(t_beta3_hat)
sum aux
scalar pval = r(mean)
di "The p-value is: " round(pval, 0.001)
```

We also could repeat this exercise with the `boottest` function from the `boottest` user-written package developed by @FischerRoodman2021 and arrive to the same conclusion.  This function works with the last estimated model, and conducts an identical procedure to that which we have done above "by hand".  Any difference in p-values is incidental, owing to different random draws.  

```{stata}
import delimited "Datasets\PorterSerra2020.csv", clear
keep if female == 1
reg econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, cluster(class_fe2)
boottest treat2016, reps(999) weight(webb) bootcluster(class_fe2) nograph
```

In principle, using such a library is likely the preferred way of conducting procedures such as the wild cluster bootstrap, however it is illustrative to see how it works in practice, as we do above.  A nice element of user-written procedures such as that of @Roodmanetal2019 is that it also seamlessly returns other quantities of interest which we would have to generate ourselves above, such as confidence interval, which we can see above where these values correspond closely to the original 95% CIs reported in the paper of [-0.015; 0.160].


Finally as a comparative exercise we may be interested in seeing how this procedure compares to a standard clustered bootstrap.  While there are many ways we could do this -- including quite easily by hand -- we examine this below using the `ClusterBootstrap` library.  It turns out that while the 95% CI on `treat2016` coming from clustered bootstrap is narrower than the 95% CI from the wild cluster bootstrap (as expected), the difference is not *so* substantial in this particular case. 

```{stata echo = 3}
qui import delimited "Datasets\PorterSerra2020.csv", clear
qui keep if female == 1
reg econmajor yr_2016 treatment_class treat2016 female_prof instate freshman american acumgpa gradeprinciples small_class, vce(bootstrap, reps(999) cluster(class_fe2))
```

## Code call-out 4.2: Exploring the Two-way Fixed Effect Model and Parameter Decompositions

**Two-Way Fixed Effects Estimators and Heterogeneous Treatment Effects** To understand the potential issues related to heterogeneous treatment effects over time and two-way fixed effect estimators, we will examine a pair of numerical examples. In particular, we will focus on the composition of the two way FE estimator $\tau$ estimated from: $$
y_{st} = \gamma_s + \lambda_t + \tau w_{st} + \varepsilon_{st}
$$ {#eq-twfe} where $y_{st}$ is the outcome variable, $\gamma_s$ and $\lambda_t$ are state (unit) and time fixed effects, $w_{st}$ is the binary treatment variable that takes the value of 1 if a state (unit) $s$ is treated at time $t$ and otherwise takes 0. We will work with a quite tractable example based on three units and 10 time periods, and will document how the approaches taken by @GoodmanBacon2018 and by @deChaisemartinDhaultfoeuille2019 to understand the two-way FE estimator compare.

The results from @GoodmanBacon2018 and those from @deChaisemartinDhaultfoeuille2019 are similar, however they take quite different paths to get there. Goodman-Bacon's (like that laid out in @AtheyImbens2018) is "mechanical" in that it is based on the underlying difference-in-differences comparisons between all groups. The result in @deChaisemartinDhaultfoeuille2019 is based on a potential outcomes frame-work, and counterfactuals under parallel trend assumptions. Thus to examine how these methods work requires somewhat different frameworks. In the case of @GoodmanBacon2018, we should consider all possible DD comparisons, while in the case of @deChaisemartinDhaultfoeuille2019 we should consider the treatment effect for each unit and time period, which requires knowing the observed and counterfactual state. While the approaches the two papers take to understand the content of the estimator differ, they refer to the same estimator, so always recover the same parameter estimate. To examine this in a more applied way, we will look at a simulated example.

To do this, let's consider a panel of 3 states/areas over the 10 years ($t$) of 2000 to 2009. One of these units is entirely untreated ($unit = 1$ or group $U$), one is treated at an early time period, 2003, ($unit = 2$ or group $k$), and the other is treated at a later time period, 2006, ($unit = 3$ or group $l$). We will construct a general structure for this data below:

```{stata}
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
list in 1/5
```

We will consider a simple-case where the actual data-generating process is known as: $$y_{unit,t} = 2 + 0.2 \times (t - 2000) + 1 \times unit + \beta_1 \times post \times unit + \beta_2 \times post \times unit \times (t - treat).$$ Here $unit$ refers to the unit number listed above (1, 2 or 3), $post$ indicates that a unit is receiving treatment in the relevant time period $t$, and $treat$ refers to the treatment period (2003 for unit 2, and 2006 for unit 3). Let's generate treatment, time to treatment, and post-treatment variables in `Stata`:

```{stata echo=7:14}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
}
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
list in 1/5
```

This specification allows for each unit to have its own fixed effect, given that $unit$ is multiplied by 1, and allows for a general time trend increasing by 0.2 units each period across the whole sample. These parameters are not so important, as what we care about are the treatment effects themselves. The impact of treatment comes from the units $\beta_1$ and $\beta_2$. The first of these, $\beta_1$, captures an immediate unit-specific jump when treatment is implemented which remains stable over time. The second of these, $\beta_2$, implies a trend break occurring *only* for the treated units once treatment comes into place. We will consider 2 cases below. In the first case $\beta_1 = 1$ and $\beta_2 = 0$ (a simple case with a constant treatment effect per unit):

```{stata echo=14:15}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
}
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
list in 1/5
```

and in a second case $\beta_1 = 1$ and $\beta_2 = 0.45$. This is a more complex case in which there are heterogeneous treatment effects over time:

```{stata echo=15:16}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
list in 1/5
```

These two cases are plotted next where the line with empty circles refers to group $U$, the line with black filled circles refers to group $k$ and the line with squares refers to group $l$

```{stata echo = 16:18}
#| code-fold: true
#| code-summary: "Show the plot code"
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
}
twoway connected y1 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y1 year if unit == 2, msymbol(O) mcolor(black) lcolor(black) || connected y1 year if unit == 3, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("(a) Simple Decomposition", position(6)) legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelA, replace)
twoway connected y2 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y2 year if unit == 2, msymbol(O) mcolor(black) lcolor(black) || connected y2 year if unit == 3, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("(b) Decomposition with trends", position(6)) legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelB, replace)
graph combine PanelA PanelB, rows(1) scheme(plottig)
qui graph export "GB_First.png", replace
```

![First Goodman-Bacon Graph](GB_First.png)

### The Two-way Fixed Effect Estimator

First we will estimate the parameter by two-way fixed effects regression. This will provide us with the parameter estimate that both @GoodmanBacon2018 and @deChaisemartinDhaultfoeuille2019 will construct in a piece-wise fashion. This is done relatively simply in `R`. We simply estimate @eq-twfe by linear regression using `lm` as laid out below:

```{stata echo=16:19}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
}
qui reg y1 i.unit i.year post
di as text "The parameter estimates by two-way fixed effects regression for the case 1 is: " _b[post]
qui reg y2 i.unit i.year post
di as text "The parameter estimates by two-way fixed effects regression for the case 2 is: " _b[post]
```

Here we see that the coefficient of interest is 2.454545. We can see that this is between the two unit-specific jumps that occur with treatment (2 and 3). We will see below why it takes this particular weighted average.

### @GoodmanBacon2018 Decomposition

Using the values simulated above, let's see how the @GoodmanBacon2018 decomposition allows us to understand estimated treatment effects. We will consider both:\
- (a) Simple Decomposition\
- (b) Decomposition with trends

The methodology @GoodmanBacon2018 decomposition suggests that we should calculate all $2 \times 2$ combinations of states and time where post-treatment units are compared to "untreated" unit (laid out at more length in the boo). In this example, this provides four specific effects, which contribute to $\widehat{\tau}$ as a weighted mean. The specific effects desired are:

-   A. $\widehat{\beta}^{2\times2}_{kU}$ from the comparison of the early treated unit with the untreated unit.\
-   B. $\widehat{\beta}^{2\times2}_{lU}$, from the comparison of the latter treated unit with the untreated unit.\
-   C. $\widehat{\beta}^{2\times2,k}_{kl}$, from the comparison of the early and latter treated units, when the early unit begin to be treated.\
-   D. $\widehat{\beta}^{2\times2,l}_{kl}$, from the comparison of the early and latter treated units, when the latter unit begin to be treated.

These will then be weighted as laid out in @GoodmanBacon2018 to provide the regression-based estimate.

#### (a) Simple Decomposition

In this case the @GoodmanBacon2018 methodology estimate $\widehat{\tau}$ weighting the next four DD comparisons

```{stata echo = 16:20}
#| code-fold: true
#| code-summary: "Show the plot code"
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
}
twoway connected y1 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y1 year if unit == 2, msymbol(O) mcolor(black%20) lcolor(black%20) || connected y1 year if unit == 3, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("A. Early Group v/s Untreated Group") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelA, replace)
twoway connected y1 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y1 year if unit == 2, msymbol(O) mcolor(black) lcolor(black) || connected y1 year if unit == 3, msymbol(Sh) mcolor(black%20) lcolor(black%20) ||, ytitle("Outcome Variable") xtitle("Time") title("B. Later Group v/s Untreated Group") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelB, replace)
twoway connected y1 year if unit == 1 & year <= 2005, msymbol(Oh) mcolor(black%20) lcolor(black%20) || connected y1 year if unit == 2 & year <= 2005, msymbol(O) mcolor(black) lcolor(black) || connected y1 year if unit == 3 & year <= 2005, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("C. Early Group v/s Later Group Before 2006") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) xlabel(2000(2)2010) name(PanelC, replace)
twoway connected y1 year if unit == 1 & year >= 2003, msymbol(Oh) mcolor(black%20) lcolor(black%20) || connected y1 year if unit == 2 & year >= 2003, msymbol(O) mcolor(black) lcolor(black) || connected y1 year if unit == 3 & year >= 2003, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("D. Early Group v/s Later Group After 2003") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) xlabel(2000(2)2010) name(PanelD, replace)
graph combine PanelA PanelB PanelC PanelD, scheme(plottig)
qui graph export "GB_Second.png", replace
```

![Second Goodman-Bacon Graph](GB_Second.png)

As seen in the plots, in the simple decomposition these effects are constants of 3 and 2 for early and later treated units given that the "treatment effect" is simply $1 \times unit$ in each case.

##### A. Early Group v/s Untreated Group

In order to calculate the effects we start making the simple DD comparison of the untreated group $U$ ($unit = 1$) with the early treated group $k$ ($unit = 3$) getting $\widehat{\beta}^{2 \times 2}_{kU}$ as $$\widehat{\beta}^{2 \times 2}_{kU} = \left( \overline{y}_k^{Post(k)} - \overline{y}_k^{Pre(k)} \right) - \left( \overline{y}_U^{Post(k)} - \overline{y}_U^{Pre(k)} \right)$$ Where $\overline{y}_k^{Post(k)}$ is the mean of the outcome variable for the early treated group $k$ ($unit = 3$) posterior to treatment, from 2003, $\overline{y}_k^{Pre(k)}$ is the mean for of the outcome variable for the early treated group $U$ ($unit = 3$) prior to treatment, (up until 2002), and $\overline{y}_U^{Post(k)}, \overline{y}_U^{Post(k)}$ are the analogous quantities for the untreated group $U$ ($unit = 1$)

```{stata echo=15:25}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
qui sum y1 if unit == 3 & post == 1
scalar y1treatmeanpost = r(mean)
qui sum y1 if unit == 3 & post == 0
scalar y1treatmeanpre = r(mean)
scalar dify1treat = y1treatmeanpost - y1treatmeanpre
qui sum y1 if unit == 1 & year >= 2003
scalar y1controlmeanpost = r(mean)
qui sum y1 if unit == 1 & year < 2003
scalar y1controlmeanpre = r(mean)
scalar dify1control = y1controlmeanpost - y1controlmeanpre
di as result round(dify1treat - dify1control, 0.01)
```

This result also can be obtained from the linear regression with the canonical DD formula $$y_{unit,t} = \alpha_0 + \alpha_1 \times Post(k) + \alpha_2 \times \mathbf{1}(unit = 3) + \beta_{kU}^{2\times2} \times Post(k) \times \mathbf{1}(unit = 3) + \varepsilon_i$$ Where $Post(k)$ indicates that the year is equal or greater than the year where the group $k$ ($unit = 3$) received the treatment (2003) and $\mathbf{1}(unit = 3)$ indicates if the observation is from the early treated group $k$ ($unit = 3$)

```{stata echo = 15:16}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
gen post2003 = (year >= 2003)
reg y1 i.post2003 i.unit i.post2003#i.unit if unit != 2
```

A third way to obtain this is from the next linear regression $$y_{unit,t} = \alpha_0 + \beta_{kU}^{2 \times 2} \times Post + \sum_{i = 2001}^{2009} \alpha_{i-2000} \times \mathbf{1}(year = i) + \alpha_{10} \times \mathbf{1}(unit = 3) + \varepsilon_i$$ Where in this case $Post$ indicates if the unit is treated (note for group $U$ this will be always 0), $\mathbf{1}(year = i)$ indicates if the observation is in period $i \in \{2001, \ldots, 2009\}$ and $\mathbf{1}(unit = 3)$ keep its meaning

```{stata echo = 15}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
reg y1 post i.year i.unit if unit != 2
```

Now we store this result for posterior use

```{stata echo = 16}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
reg y1 post i.year i.unit if unit != 2
}
scalar bku = _b[post]
```

##### B. Later Group v/s Untreated Group

The next DD comparison we calculate is that which compares the later treated group $l$ ($unit = 2$) with the untreated group $U$ ($unit = 1$), resulting in $\widehat{\beta}^{2 \times 2}_{lU}$. As above, we can generate this DD estimate in a number of ways (most simply by double-differencing with means), and this will then be stored.

```{stata echo = 15:26}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
qui sum y1 if unit == 2 & post == 1
scalar y1treatmeanpost = r(mean)
qui sum y1 if unit == 2 & post == 0
scalar y1treatmeanpre = r(mean)
scalar dify1treat = y1treatmeanpost - y1treatmeanpre
qui sum y1 if unit == 1 & year >= 2006
scalar y1controlmeanpost = r(mean)
qui sum y1 if unit == 1 & year < 2006
scalar y1controlmeanpre = r(mean)
scalar dify1control = y1controlmeanpost - y1controlmeanpre
scalar blu = dify1treat - dify1control
di as result round(blu, 0.01)
```

##### C. Early Group v/s Later Group Before 2006

Next we calculate the effects from the DD comparisons of early and later treated groups, up until the later treated group receives treatment (2006). This is: $$\widehat{\beta}^{2 \times 2, k}_{kl} \equiv \left( \overline{y}^{Mid(k,l)}_{k} - \overline{y}^{Pre(k)}_{k} \right) - \left( \overline{y}^{Mid(k,l)}_{l} - \overline{y}^{Pre(k)}_{l} \right)$$ where $\overline{y}^{Mid(k,l)}_{k}$ is the mean of the outcome variable for the early treated group $k$ ($unit = 3$) in the period between the treatment for the group $k$ and the group $l$ ($unit = 2$), from 2003 to 2005, $\overline{y}^{Pre(k)}_{k}$ is the mean for of the outcome variable for the early treated group $k$ ($unit = 3$) previous to treatment, until 2002, and $\overline{y}^{Mid(k,l)}_{l}, \overline{y}^{Pre(k)}_{l}$ are the analogous for the later treated group $l$ ($unit = 2$)

```{stata echo = 15:17}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
reg y1 post i.year i.unit if unit != 1 & year < 2006
scalar bklk = _b[post]
di round(bklk, 0.01)
```

##### D. Early Group v/s Later Group After 2003

The last DD comparison is for early and later treated groups, starting from 2006 $$\widehat{\beta}^{2 \times 2, l}_{kl} \equiv \left( \overline{y}^{Post(l)}_{l} - \overline{y}^{Mid(k,l)}_{l} \right) - \left( \overline{y}^{Post(l)}_{k} - \overline{y}^{Mid(k,l)}_{k} \right)$$ Where $\overline{y}^{Post(l)}_{l}$ is the mean of the outcome variable for the later treated group $l$ ($unit = 2$) in the period after this group received the treatment, from 2006, $\overline{y}^{Mid(k,l)}_{l}$ is the mean for of the outcome variable for the later treated group $l$ ($unit = 2$) in the period between the treatment for the group $k$ ($unit = 3$) and the group $l$, from 2003 to 2005, and $\overline{y}^{Post(l)}_{k}, \overline{y}^{Mid(k,l)}_{k}$ are the analogous quantities for the early treated group $k$ ($unit = 3$). We can generate and save this quantity as we have previously:

```{stata echo = 15:17}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
reg y1 post i.year i.unit if unit != 1 & year > 2002
scalar bkll = _b[post]
di round(bkll, 0.01)
```

This comparison is the comparison which can potentially result in undesired results if treatment effects are dynamic over time because it views group 3 (the previously treated group) as a control. However, in this case, given that treatment effects are homogenous over time we do not have a major problem here, and we observe that $\widehat{\beta}^{2 \times 2, l}_{kl}=2$.

##### Weights

We can now arrive to the OLS estimate of this two-way fixed effect model by generating the weighted mean of the previous estimates as: $$\widehat{\tau} = W_{kU} \cdot \widehat{\beta}^{2\times 2}_{kU} + W_{lU} \cdot \widehat{\beta}^{2\times 2}_{lU} + W_{kl}^{k} \cdot \widehat{\beta}^{2\times 2,k}_{kl} + W_{kl}^{l} \cdot \widehat{\beta}^{2\times 2,l}_{kl}$$ Where each $W$ is the weight that the respective $\beta$ has in this weighted mean, specifically: \begin{align*} 
W_{kU} & = \frac{(n_k + n_U)^2\widehat{V}^D_{kU}}{\widehat{V}^D} \quad &  \quad W_{lU} & = \frac{(n_l + n_U)^2\widehat{V}^D_{lU}}{\widehat{V}^D} \\ 
W_{kl}^k & = \frac{[(n_k + n_l)(1 - \overline{D}_l)]^2\widehat{V}^{D,k}_{kl}}{\widehat{V}^D} \quad &  \quad W_{kl}^l & = \frac{[(n_k + n_l)(1 - \overline{D}_k)]^2\widehat{V}^{D,l}_{kl}}{\widehat{V}^D}
\end{align*} Where $n$ refers to the sample share of the group

```{stata}
scalar nk = 1/3
scalar nl = 1/3
scalar nu = 1/3
```

$\overline{D}$ referes to the share of time the group is treated

```{stata echo = 15:18}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
qui sum post if unit == 3
scalar Dk = r(mean)
qui sum post if unit == 2
scalar Dl = r(mean)
```

and $\widehat{V}$ refers to how much treatment varies

```{stata echo = 15:27}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
qui sum post if unit == 3
scalar Dk = r(mean)
qui sum post if unit == 2
scalar Dl = r(mean)
scalar VkU = 0.5*0.5*(Dk)*(1-Dk)
scalar VlU = 0.5*0.5*(Dl)*(1-Dl)
scalar Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
scalar Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
qui reg post i.unit i.year
predict residuals, residuals
gen residuals2 = residuals^2
qui sum residuals2
scalar VD = r(mean)
```

The weights are thus the following:

```{stata echo = 31:38}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
qui sum post if unit == 3
scalar Dk = r(mean)
qui sum post if unit == 2
scalar Dl = r(mean)
scalar VkU = 0.5*0.5*(Dk)*(1-Dk)
scalar VlU = 0.5*0.5*(Dl)*(1-Dl)
scalar Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
scalar Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
qui reg post i.unit i.year
predict residuals, residuals
gen residuals2 = residuals^2
qui sum residuals2
scalar VD = r(mean)
scalar nk = 1/3
scalar nl = 1/3
scalar nu = 1/3
}
scalar wkU = ((nk + nu)^2*VkU)/VD
di wkU
scalar wlU = ((nl + nu)^2*VlU)/VD
di wlU
scalar wklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD
di wklk
scalar wkll = (((nk + nl)*Dk)^2*Vkll)/VD
di wkll
```

With this in mind the $\tau$ estimate is

```{stata echo = 47:48}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
qui sum post if unit == 3
scalar Dk = r(mean)
qui sum post if unit == 2
scalar Dl = r(mean)
scalar VkU = 0.5*0.5*(Dk)*(1-Dk)
scalar VlU = 0.5*0.5*(Dl)*(1-Dl)
scalar Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
scalar Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
qui reg post i.unit i.year
predict residuals, residuals
gen residuals2 = residuals^2
qui sum residuals2
scalar VD = r(mean)
scalar nk = 1/3
scalar nl = 1/3
scalar nu = 1/3
scalar wkU = ((nk + nu)^2*VkU)/VD
di wkU
scalar wlU = ((nl + nu)^2*VlU)/VD
di wlU
scalar wklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD
di wklk
scalar wkll = (((nk + nl)*Dk)^2*Vkll)/VD
di wkll
reg y1 post i.year i.unit if unit != 2
scalar bku = _b[post]
reg y1 post i.year i.unit if unit != 3
scalar blu = _b[post]
reg y1 post i.year i.unit if unit != 1 & year < 2006
scalar bklk = _b[post]
reg y1 post i.year i.unit if unit != 1 & year > 2002
scalar bkll = _b[post]
}
scalar tau = wkU * bku + wlU * blu + wklk * bklk + wkll * bkll
di tau
```

as observed in the two-way fixed effect estimate above.

#### (b) Decomposition with trends

In this case the @GoodmanBacon2018 decomposition follows as above generating the treatment effect as follows:

```{stata echo = 17:20}
#| code-fold: true
#| code-summary: "Show the plot code"
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
}
twoway connected y2 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y2 year if unit == 2, msymbol(O) mcolor(black%20) lcolor(black%20) || connected y2 year if unit == 3, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("A. Early Group v/s Untreated Group") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelA, replace)
twoway connected y2 year if unit == 1, msymbol(Oh) mcolor(black) lcolor(black) || connected y2 year if unit == 2, msymbol(O) mcolor(black) lcolor(black) || connected y2 year if unit == 3, msymbol(Sh) mcolor(black%20) lcolor(black%20) ||, ytitle("Outcome Variable") xtitle("Time") title("B. Later Group v/s Untreated Group") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) name(PanelB, replace)
twoway connected y2 year if unit == 1 & year <= 2005, msymbol(Oh) mcolor(black%20) lcolor(black%20) || connected y2 year if unit == 2 & year <= 2005, msymbol(O) mcolor(black) lcolor(black) || connected y2 year if unit == 3 & year <= 2005, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("C. Early Group v/s Later Group Before 2006") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) xlabel(2000(2)2010) name(PanelC, replace)
twoway connected y2 year if unit == 1 & year >= 2003, msymbol(Oh) mcolor(black%20) lcolor(black%20) || connected y2 year if unit == 2 & year >= 2003, msymbol(O) mcolor(black) lcolor(black) || connected y2 year if unit == 3 & year >= 2003, msymbol(Sh) mcolor(black) lcolor(black) ||, ytitle("Outcome Variable") xtitle("Time") title("D. Early Group v/s Later Group After 2003") legend(off) scheme(plottig) xline(2002, lpattern(dash) lcolor(red)) xline(2005, lpattern(dash) lcolor(red)) xlabel(2000(2)2010) name(PanelD, replace)
graph combine PanelA PanelB PanelC PanelD, scheme(plottig)
qui graph export "GB_Third.png", replace
```

![Third Goodman-Bacon Graph](GB_Third.png)

As seen in the plots, in the decomposition with trends these effects are no longer constants of 3 and 2 for early and later treated units given that the "treatment effect" is no longer simply $1 \times unit$ in each case.

```{stata echo = 15:54}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
}
* 2x2 DD Regressions for betas
qui reg y2 post i.year i.unit if unit != 2
scalar bku = _b[post]
qui reg y2 post i.year i.unit if unit != 3
scalar blu = _b[post]
qui reg y2 post i.year i.unit if unit != 1 & year < 2006
scalar bklk = _b[post]
qui reg y2 post i.year i.unit if unit != 1 & year > 2002
scalar bkll = _b[post]
* Share of time treated
qui sum post if unit == 3
scalar Dk = r(mean)
qui sum post if unit == 2
scalar Dl = r(mean)
* How much treatment varies
scalar VkU = 0.5*0.5*(Dk)*(1-Dk)
scalar VlU = 0.5*0.5*(Dl)*(1-Dl)
scalar Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
scalar Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
qui reg post i.unit i.year
predict residuals, residuals
gen residuals2 = residuals^2
qui sum residuals2
scalar VD = r(mean)
* Share of sample
scalar nk = 1/3
scalar nl = 1/3
scalar nu = 1/3
* Weights
scalar wkU = ((nk + nu)^2*VkU)/VD
di wkU
scalar wlU = ((nl + nu)^2*VlU)/VD
di wlU
scalar wklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD
di wklk
scalar wkll = (((nk + nl)*Dk)^2*Vkll)/VD
di wkll
* Tau
scalar tau = wkU * bku + wlU * blu + wklk * bklk + wkll * bkll
di tau
```

What is noteworthy here is the surprising behaviour flagged by @GoodmanBacon2018 for the final comparison based on the case where the earlier treated unit (unit 3) is used as a control for the later trated unit (unit 2). In this case, given that there *are* time-varying treatment effects, despite the fact that each unit-specific treatment effect is positive, we observe that the parameter $\widehat{\beta}^{2 \times 2, l}_{kl}$ is actually *negative*. In this particular example this negative value (-1.375) is not sufficient to turn the weighted treatment effect estimate negative, but if you play around with the size of the parameters $\beta_1$ and $\beta_2$ above, you will see that large enough differences in trends *can* result in such estimates! Here, as above, we see that when we aggregate unit-specific estimates as `tau`, the estimate (by definition) agrees with the estimate generated by two-way fixed effect models previously.

### @deChaisemartinDhaultfoeuille2019's Procedure

Now, we will show that the procedures described in @deChaisemartinDhaultfoeuille2019, despite arriving to the estimator in a different way, also let us understand how the regression weights the two-way fixed effect estimator. In this case, rather than considering each treatment-control comparison pair, the authors note that the two-way fixed estimator can be conceived as a weighted sum of each single group by time period in any post-treatment group.

The authors define $\widehat{\beta}_{fe}$ as the coefficient estimated in the following (standard) two-way fixed effects regression: $$y_{i,s,t} = \beta_0 + \beta_{fe} D_{s,t} + \mu_s + \lambda_t + \varepsilon_{s,t}$$ Where $D_{s,t}$ is the mean over $i$ of a binary indicator variable that takes value of 1 if the unit $i$ in state $s$ is treated at period $t$ and 0 otherwise, in our case as we have one observartion per state $D_{s,t} = post_{s,t}$, meanwhile $\mu_s$ and $\lambda_t$ are state and time fixed effects. This is, of course, precisely the same model as we have estimated in @eq-twfe, implying that $\beta_{fe}=2.4545$ in cases without post-treatment trends (`y1`), or $\beta_{fe}=3.8045$ in cases with post-treatment dynamics (`y2`).

@deChaisemartinDhaultfoeuille2019 define the ATE for any ($s,t$) cell as: $$\Delta_{s,t} = \frac{1}{N_{s,t}} \sum_{i = 1}^{N_{s,t}}[Y_{i,s,t}(1) - Y_{i,s,t}(0)].$$ You will note that here we require an unobserved counterfactual $Y_{i,s,t}(0)$. If we impose a parallel trend assumption, such a counterfactual can be inferred from unit-specific fixed effects, time-specific fixed effects, and the constant term. Because in this case we *know* our data generating process, we can simply generate this counterfactual as the data generating process, absent any effect of treatment. Below we generate such a counterfactual, where you will note that we impose that this is an 'untreated' counterfactual by setting the treatment effects to 0 in the generation of `y1_c` below:

```{stata echo = 15}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
}
gen y1_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
```

It is likely useful to confirm to ourselves that graphically we are indeed generating the untreated counterfactual in this way.

```{stata echo = 16:18}
#| code-fold: true
#| code-summary: "Show the plot code"
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y1_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
}
line y1 y1_c year if unit == 2, scheme(plottig) ytitle("Y") xtitle("Year") lwidth(thick thick) lpattern(solid dash) lcolor(blue red) legend(off) text(7 2007 "Y(1)" 5 2007 "Y(0)") title("(a) Unit 2 Outcome and Counterfactual", position(6)) name(unit2, replace)
line y1 y1_c year if unit == 3, scheme(plottig) ytitle("Y") xtitle("Year") lwidth(thick thick) lpattern(solid dash) lcolor(blue red) legend(off) text(9 2007 "Y(1)" 6 2007 "Y(0)") title("(b) Unit 3 Outcome and Counterfactual", position(6)) name(unit3, replace)
graph combine unit2 unit3, rows(1) scheme(plottig)
qui graph export "deChaisemartinDhaultfoeuille.png", replace
```

![deChaisemartinDhaultfoeuille](deChaisemartinDhaultfoeuille.png)

This allows us to calculate a state- and time-period specific treatment effect ($\Delta_{s,t}$) for each treated unit. We do so, calculating this quantity for all units in which treatment exists:

```{stata echo = 16:17}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y1_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
}
gen Delta_st = y1 - y1_c if post == 1
list y1 y1_c unit year Delta_st if post == 1, sepby
```

Unsurprisingly, given the data generating process we have defined, we see that each treatment effect is 2 for unit 2, and 3 for unit 3. If we were to calculate a mean treatment effect by hand, we may wish to simply take an average over all periods and units. However, one of the key results of @deChaisemartinDhaultfoeuille2019 is to show that under a series of standard assumptions $$\beta_{fe} = E \left[ \sum_{s,t:D_{s,t}=1}\frac{N_{s,t}}{N_1}w_{s,t}\Delta_{s,t} \right]$$ Where $N_1$ refers to the sum of all treated observations and $$w_{s,t} = \frac{\varepsilon_{s,t}}{\sum_{s,t:D_{s,t}=1}\frac{N_{s,t}}{N_1}\varepsilon_{s,t}}$$ Where $\varepsilon_{s,t}$ is the residual from a regression of $D_{s,t}$ on state and time fixed-effects. To confirm this in our data, we will estimate these regression residuals and add them into the dataframe:

```{stata echo = 17:25}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y1_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
gen Delta_st = y1 - y1_c if post == 1
}
quietly{
reg post i.unit i.year
predict eps_st, residuals
replace eps_st = . if post != 1
sum eps_st
gen w_st = eps_st / r(sum)
format w_st %8.7f
}
list y1 y1_c unit year Delta_st w_st if post == 1, sepby(unit)
```

Note here that after generating $w_{s,t}$ we print this out using the round function to avoid very small digits appearing which are only different to zero given machine precision. The key thing that we can see is that the effective weighting of treatment effects which occurs in regression is quite different to what we would expect. Indeed, four periods are given 0 weights! Finally, we can confirm that this decomposition gives us the two-way fixed effect estimate by multiplying $\Delta_{s,t}$ and $w_{s,t}$ and summing:

```{stata echo = 22:24}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y1 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0 * post * unit * time
gen y1_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
gen Delta_st = y1 - y1_c if post == 1
reg post i.unit i.year
predict eps_st, residuals
replace eps_st = . if post != 1
sum eps_st
gen w_st = eps_st / r(sum)
}
gen Delta_times_w = Delta_st * w_st
sum Delta_times_w
di "de Chaisemartin and Xavier D'Haultfoeuille's decomposition returns an estimates of: " r(sum)
```

We can see that correctly, this decomposition also returns the two-way fixed effect estimate of 2.4545.

We can follow precisely the same series of steps to see the case of the decomposition where treatment exposition also results in a trend-break. To see this, we conduct each of the above steps below, however here we have not produced similar graphs (though you may wish to do so to confirm that counterfactuals make sense):

```{stata echo = 21:23}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
reg post i.unit i.year
predict eps_st, residuals
replace eps_st = . if post != 1
sum eps_st
gen w_st = eps_st / r(sum)
format %8.7f w_st
}
gen y2_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
gen Delta_st2 = y2 - y2_c if post == 1
list y2 y2_c unit year Delta_st2 w_st if post == 1, sepby(unit)
```

Because there is no difference in the structure of the treatment indicator or the unit and time fixed effects, the residuals $w_{s,t}$ are identical, though of course the treatment effects themselves, $\Delta_{s,t}$ are not. Thus, once again we see that later treatment effects for unit 3 (precisely those units for which treatment effects are largest), are given zero weights. Finally, again we can calculate the two-way fixed effect estimate following this decomposition by summing across units, capturing the estimate we have previously observed in regression models of 3.804545.

```{stata echo = 22:24}
quietly{
set obs 30
gen obs  = _n
gen unit = ceil(obs/10)
bys unit: gen year = _n+1999
gen treat = 2006 if unit==2
replace treat = 2003 if unit==3
gen time = year - treat
gen post = time>=0 & time!=.
replace treat = 0 if treat==.
replace time  = 0 if time==.
replace post  = 0 if post==.
gen y2 = 2 + (year - 2000) * 0.2 + 1 * unit + 1 * post * unit + 0.45 * post * unit * time
reg post i.unit i.year
predict eps_st, residuals
replace eps_st = . if post != 1
sum eps_st
gen w_st = eps_st / r(sum)
gen y2_c = 2 + (year - 2000) * 0.2 + 1 * unit + 0 * post * unit + 0 * post * unit * (time)
gen Delta_st2 = y2 - y2_c if post == 1
}
gen Delta_times_w = Delta_st2 * w_st
sum Delta_times_w
di "de Chaisemartin and Xavier D'Haultfoeuille's decomposition returns an estimates of: " r(sum)
```

Depending on the nature of treatment assignment, ie the number of treated periods, as well as the period in which treatment is adopted in different units, these weights will vary, and can even be negative. You may wish to explore alternative set-ups and confirm to yourself that this is the case, and see that regardless of the nature of the setting, both @GoodmanBacon2018 and @deChaisemartinDhaultfoeuille2019's decompositions recover the two-way fixed effect estimate.

## Code call-out 4.3: Event study and Interaction-weighted Estimators

## Code call-out 4.4: Synthetic control, difference-in-differences, and synthetic difference-in-differences
In this code call out, we will explore the use of synthetic control methods as well as extensions into synthetic difference-in-differences with the data using in the original @Abadieetal2010 paper.  In particular, these data provide a balanced sample from 39 states in the United States covering the period of 1970 to 2000.  In particular, the interest in these methods is estimating the impact of the passage of Proposition 99, which was a reform to increase the sales tax paid per package of cigarettes sold in California.

We will begin by opening the data used by @Abadieetal2010, and checking the variables available.  If we wished we could confirm that this is effectively a balanced panel by tabulating (`tab`) the variable year and state.  

```{stata}
use "Datasets/smoking.dta"
describe
```

Here our outcome of interest will be the variable `cigsale` which records the number of packages sold per capita in each state.  The proposition 99 reform was passed in 1989, and we will thus consider 1970-1988 as the pre-period, while the period of 1989-2000 is the treatment period. While @Abadieetal2010 actually construct their synthetic control considering both certain pre-period realisations as well as control variables included in their data, we will follow suggestions from @Fermanetal2020 and document a case where we simply use the full set of pre-treatment realisations of the outcome of interest to generate our synthetic control.  We will do this below, noting that the `state` variable in these data uses Stata FIPS codes, and so California corresponds to state 3.  The below code assumes that the `synth` routine is installed, which is available from Stata's SSC.

<!--
The synthetic control method seeks to construct a "synthetic control" for a treated unit (in this case, California) using a weighted combination of control units (other states). The aim is for this synthetic control to closely resemble the treated unit in the pre-treatment period based on predictor variables.

-   Use the 'synth' command to construct the synthetic control for California
-   Predictor variables: cigsale from specific years, beer, lnincome, retprice, age15to24
-   Treated unit: California (state==3)
-   Treatment period: 1989
-   Periods used to construct the synthetic control: 1980-1988

Once the synthetic control is constructed, we can compare the trends of the treated unit and the synthetic control in the post-treatment period. Any divergence in trends is interpreted as the treatment effect. In this case, we are assessing the impact of a hypothetical policy implemented in California in 1989 on cigarette sales.
-->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(Statamarkdown)
```

```{stata}
use "Datasets/smoking.dta"

// Set the data as panel data with 'state' as the panel unit and 'year' as the time variable
tsset state year

synth cigsale cigsale(1970) cigsale(1971) cigsale(1972) cigsale(1973) ///
              cigsale(1974) cigsale(1985) cigsale(1976) cigsale(1977) ///
              cigsale(1978) cigsale(1979) cigsale(1980) cigsale(1981) ///
              cigsale(1982) cigsale(1983) cigsale(1984) cigsale(1985) ///
              cigsale(1986) cigsale(1987) cigsale(1988),              ///
   trunit(3) trperiod(1989) keep(synth_results) replace
```

You may note a number of things with the way that we have implemented `synth` above.  The first is that the way we have entered the covariates on which to match (pre-treatment lags of the sales variable) is very cumbersome.  It turns out that this is required if we do indeed wish to match on the variable at each pre-treatment period.  While the syntax `cigsale(1970(1)1988)` would also be valid, it is *not* what we are after, as this would match on mean sales across the whole period, rather than sales in each pre-treatment period.  A second thing to note is that there is substantial output both in terms of the data setup, the root mean square prediction error across all variables used in the match, and the resulting weights and predictor balances.   From this we can see (for example), that the synthetic control for California is constructed using a combination of Colorado, Connecticut, Montana, Nevada, New Hampshire, and Utah.  In the above implementation we have saved a dataset called `synth_results` which we can use to consider how the resulting synthetic control compares with California.  The nature of the variables generated in this dataset are contained in the help file from `synth`.   We will generate this graphical output below, starting with the `synth_results` dataset.

```{stata echo = 1:10}
use synth_results.dta, clear
tsset _time

* Create the graph with a dotted line at the treatment period
set scheme plotplainblind
twoway tsline _Y_treated _Y_synthetic, lcolor(blue red) lwidth(thick thick) ///
    legend(order(1 "California" 2 "Synthetic California") pos(1) ring(0))   ///
    xline(1989)  ytitle("Cigarette Sales") xtitle("Year")
qui graph export "synth.png", replace

* 1.
* 2. The RMSPE (Root Mean Squared Prediction Error) is a measure of how well the synthetic control approximates California in the pre-treatment period. A lower RMSPE indicates a better fit. In this case, the RMSPE is 1.756235, suggesting a reasonably good fit.
* 3. The "Predictor Balance" table shows how California and the synthetic control compare in terms of the predictor variables. The figures show that there is a good balance between the treated unit and the synthetic control on these variables.

* The graph displays per capita cigarette sales in California and the synthetic control over time. The divergence between the two lines post-1989 represents the estimated effect of the policy.
```
![California and it's Synthetic Control](synth.png)

Here we can see that (as expected) the synthetic control and California follow a very similar trend up to the period in which treatment is applied.  This comes precisely from the optimisation procedure, which seeks to construct a synthetic control which minimises this distance.  However, we observe that outcomes then diverge between California and the synthetic control in the post-reform period, with a substantially larger decline in California.  If we wished to formally conduct hypothesis tests related to this synthetic control procedure, we could conduct the permutation inference procedures laid out in @Abadieetal2010 and discussed in Chapter 4. While we will not set this up here it is a worthwhile activity to understand the practicalities of inference.  We *will* also consider below extensions of these methods into synthetic difference-in-differences, additionally documenting inference following permutation procedures.

We will do this using the `sdid` library...

```{stata}
qui use "Datasets/smoking.dta"
gen treated = state==3&year>=1989
decode state, gen(statename)

sdid cigsale statename year treated, method(sdid) vce(placebo) ///
graph g1on g1_opt(ylabel(-110(20)50) xtitle("")) ///
g2_opt(ylabel(0(25)150) ytitle("Packs per capita")) 
```
::: {#fig-sdid layout-ncol=2}

![Trends and Time Weights](sdid_trends1989.png){#fig-surus}

![Differences and State Weights](sdid_weights1989.png){#fig-hanno}

Synthetic difference-in-differences: Unit weights, time weights, trends and differences
:::


```{stata echo=3:6}
qui use "Datasets/smoking.dta"
gen treated = state==3&year>=1989
decode state, gen(statename)
sdid cigsale statename year treated, method(sc) vce(placebo) ///
graph g1on g1_opt(ylabel(-110(20)50) xtitle("")) ///
g2_opt(ylabel(0(25)150) ytitle("Packs per capita"))
```
::: {#fig-sc layout-ncol=2}

![Trends and Time Weights](sc_trends1989.png){#fig-surus}

![Differences and State Weights](sc_weights1989.png){#fig-hanno}

Synthetic control: Unit weights, time weights, trends and differences
:::


```{stata echo=3:6}
qui use "Datasets/smoking.dta"
gen treated = state==3&year>=1989
decode state, gen(statename)
sdid cigsale statename year treated, method(did) vce(placebo) ///
graph g1on g1_opt(ylabel(-110(20)50) xtitle("")) ///
g2_opt(ylabel(0(25)150) ytitle("Packs per capita")) graph_export(did_, .png)
```
::: {#fig-did layout-ncol=2}

![Trends and Time Weights](did_trends1989.png){#fig-surus}

![Differences and State Weights](did_weights1989.png){#fig-hanno}

Difference-in-differences: Unit weights, time weights, trends and differences
:::

