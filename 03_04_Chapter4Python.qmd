---
title: "Chapter 4"
bibliography: references.bib
---

# Code Call Outs

## Code Call Out 4.1: Wild cluster bootstrap implementation

To see the difference between the wild cluster bootstrap described in Section 4.2.3.2 of the book and other clustering options such as standard cluster bootstrap or clustered standard errors we will set up an example by hand of the wild cluster bootstrap.  We will do this with data provided by @PorterSerra2020 who conducted a field experiment which sought to test whether student exposure to engaging and successful women instructors in early economics classes increases the likelihood that female students go on to major in economics.  The dataset is provided under the name `PorterSerra2020.csv`, and we will open this data as `data` below:

```{python}
import pandas as pd
data = pd.read_csv("Datasets/PorterSerra2020.csv", sep = ";")
```

Following @PorterSerra2020 we will estimate the following linear probability model (LPM)
$$Y_{i} = \beta_0 + \beta_1 dt_i + \beta_2 dT_i + \beta_3 dt_i \times dT_i + \delta \mathbf{X}_i + \varepsilon_i$$
where we use identical notation from their paper.  Treatment was randomly applied at the class level in 2016, and classes also existed in 2015, but no treatment was applied.  Above, $Y_i$ is a student's (binary) decision of whether or not to major in economics (`econmajor`), $dt_i$ (`yr_2016`) a dummy equal to one if she took the class in 2016 and zero if she took a class in 2015, and $dT_i$ (`treatment_class`) is a dummy equal to one if she is in a treatment class, and zero if she is in a control class. The interaction between these two dummies (`treat2016`) is the coefficient of interest and $\mathbf{X}_i$ is a vector of individual, demographic and class controls such as if the course was taught by a female professor (`female_prof`), if the student is an in-state student (`instate`), if the student is in freshman year (`freshman`), if the student is american (`american`), the student's cumulative GPA (`ACumGPA`), the student's grade in their Principles of Economics course (`gradePrinciples`) and if the student take a class with a limit of 40 students (`small_class`).  As treatment is assigned at the class level (`class_fe2`), and as there are few clusters (12 clusters), the authors proceed to conduct inference using a wild cluster bootstrap.  We conduct this procedure below.

Here in particular we are interested in the parameter $\beta_3$ which under difference-in-difference assumptions will identify the effect of female role models on future enrollment in an economics major.  Before examining this process, we will estimate the LPM in order to get an estimate of the coefficient of interest $\widehat{\beta}_3$, along with the (traditional) cluster-robust standard error $se\left(\widehat{\beta}_3\right)$, and resulting $t$-statistic for the test of a null effect: $t=\left(\widehat{\beta}_3 - 0\right)/se\left(\widehat{\beta}_3\right)$.  We will do this using the `Regpyhdfe` function from the `regpyhdfe` package to estimate our regression model. The usage of this function includes a several arguments separate from each other, one for the dependent variable, other for the explanatory covariates, or predictors, other for the clustering variables and other for one of the the variables that determine the fixed effects (or ommitted if no fixed effects are desired). As we are not estimating with fixed effects or IV we don't indicate an `ids` argument:

```{python}
from regpyhdfe import Regpyhdfe
data['Constant'] = 1
LPM = Regpyhdfe(df = data[data['female'] == 1], target = 'econmajor', 
                       predictors = ['yr_2016', 'treatment_class', 'treat2016',
                                     'female_prof', 'instate', 'freshman',
                                     'american', 'ACumGPA', 'gradePrinciples',
                                     'small_class', 'Constant'],
                       cluster_ids=['class_fe2']).fit()
print(LPM.summary2())
```

It can be seen that the coefficient of interest is 0.0801 (as per column 4 of Table 4 of @PorterSerra2020) with a cluster-robust standard error of 0.0365 and a resulting t-statistic of 2.197. Below, we store these values along with the residuals of this unrestricte regression $\widehat{\varepsilon}$:

```{python}
beta3_hat = LPM.params['treat2016']
se_beta3_hat = LPM.bse['treat2016']
t_beta3_hat = LPM.tvalues['treat2016']
eps_hat = LPM.resid
```

Because we are interested in considering the variation of data in a model where we assume the null hypothesis $\beta_3=0$ is true, we will now *impose* this hypothesis, and re-estimate our model.  We do this below, imposing the restriction $\beta_3 = 0$ by simply omiting the `treat2016` variable from the model, storing the restricted residuals from this regression as $\tilde{\varepsilon}$. 

```{python}
LPM_r = Regpyhdfe(df = data[data['female'] == 1], target = 'econmajor',
                  predictors = ['yr_2016', 'treatment_class', 'female_prof', 
                                'instate', 'freshman', 'american', 'ACumGPA', 
                                'gradePrinciples', 'small_class', 'Constant'],
                  cluster_ids=['class_fe2']).fit()
eps_tilde = LPM_r.resid
```

These restricted residuals `eps_tilde` above will be key in our wild cluster bootstrap procedure.  For a given bootstrap replication, for each cluster we will assign a value of -1 or +1, and multiply the previous residuals by this (cluster-specific) value.  This will maintain correlations between residuals fixed within each cluster, but allow correlations to vary between clusters.  We will thus generate a new "sample" of data taking original data and updated residuals, resulting in a new outcome for $Y_i$.

Below we will initialise this wild cluster bootstrap procedure, setting some large amount of bootstrap replicates (here 999), before storing the data we need as `bsample`.  We will then also incorporate the residuals from above into this dataframe, so `bsample` contains all relevant covariates, as well as the restricted residuals.  It is worth noting, that in practice, all we require from these covariates is the ability to form $\widehat{Y}_i=\widehat\beta_0+\widehat\beta_1 dt_i + \widehat\beta_2 dT_i + \widehat\delta \mathbf{X}_i$, and we could actually just work with the quantity $\widehat{Y}_i$ below (you may wish to confirm this to yourself by editing the code below).  However, for ease of exposition we will work with the full set of covariates in code below, even though this is somewhat less efficient.

```{python}
B = 999
WildClusterBootstrap = pd.DataFrame({'beta3': [float('nan')] * B,
                                     'se_beta3': [float('nan')] * B,
                                     't_stat': [float('nan')] * B})
bsample = data[data['female'] == 1][['econmajor', 'yr_2016', 'treatment_class', 
                                     'treat2016', 'female_prof', 'instate', 
                                     'freshman', 'american', 'ACumGPA', 
                                     'gradePrinciples', 'small_class', 
                                     'class_fe2', 'Constant']].reset_index(drop = False)
bsample['eps_tilde'] = eps_tilde
```

Now let's see what each iteration of a wild cluster bootstrap looks like.  As we will generate our new sample of data by (randomly) selecting values of -1 or 1 for each cluster to form "resampled" residuals, we will start by drawing these "Rademacher" weights for each cluster.  Below we do this by first generating a cluster-specific draw for each cluster $g$ which assigns $a_g = 1$ or $a_g = -1$ with probability 0.5 (as seen in `clusters`).  This value $a_g$ is joined into our main data:
```{python}
import random
clusters = pd.DataFrame({'class_fe2': bsample['class_fe2'].unique(),
                         'ag': random.choices([-1, 1], k = 12)})
print(clusters)
bsample = pd.merge(left = bsample, right = clusters, how = 'left',
                   on = 'class_fe2')
```
Now, based on this draw and the original errors from the restricted model, we will generate the new set of bootstrap errors, which below we call `berrors`:
```{python}
bsample['berrors'] = bsample['eps_tilde'] * bsample['ag']
```
Finally, below we will generate our new resampled outcome variable `beconmajor` from covariates, restricted regression estimates, and our resampled error term `berrors`.
```{python}
bsample['beconmajor'] = (LPM_r.params['Constant'] + 
                         LPM_r.params['yr_2016'] * bsample['yr_2016'] +
                         LPM_r.params['treatment_class'] * bsample['treatment_class'] +
                         LPM_r.params['female_prof'] * bsample['female_prof'] +
                         LPM_r.params['instate'] * bsample['instate'] +
                         LPM_r.params['freshman'] * bsample['freshman'] +
                         LPM_r.params['american'] * bsample['american'] +
                         LPM_r.params['ACumGPA'] * bsample['ACumGPA'] +
                         LPM_r.params['gradePrinciples'] * bsample['gradePrinciples'] +
                         LPM_r.params['small_class'] * bsample['small_class'] +
                         bsample['berrors'])
```

With this data in hand, we estimate the non-restricted model exactly as we did so previously with `Regpyhdfe`.  Below, we estimate this model, and examine summary output:

```{python}
LPM_b = Regpyhdfe(df = bsample, target = 'beconmajor',
                  predictors = ['yr_2016', 'treatment_class', 'female_prof', 
                                'instate', 'freshman', 'american', 'ACumGPA', 
                                'gradePrinciples', 'small_class', 'Constant',
                                'treat2016'],
                  cluster_ids = ['class_fe2']).fit()
print(LPM_b.summary2())
```

You will note here that the coefficient of interest (that on `treat2016`) is small and insignificant.  This should not be surprising to us, as we have imposed that this coefficient should be zero in the process where we generated `beconmajor` previously.  The idea of this process is that in this way we should have some idea of the variation we may expect in parameter estimates when the true parameter actually *is* zero.  If we observe that our true estimate greatly exceeds these "null" estimates, we may be willing to conclude that the original effect is real.  We store the relevant values from our regression model below to calculate a t-statistic from this bootstrap replicate.

```{python}
WildClusterBootstrap.loc[0,'beta3']    = LPM_b.params['treat2016']
WildClusterBootstrap.loc[0,'se_beta3'] = LPM_b.bse['treat2016']
WildClusterBootstrap.loc[0,'t_stat']   = LPM_b.tvalues['treat2016']
```
We wish to see how extreme our original t-statistic is compared to many t-statistics generated in this way, where the null is imposed.  Thus, we will now repeat the previous bootstrap replicate $B-1$ more times in a loop, so that we have $B$ t-statistics.

```{python}
for b in range(1,B):
  # Erase from common data frame the data of previous replication
  bsample = bsample.drop(columns = ['ag', 'berrors', 'beconmajor'])
  # Add new replication data
  clusters = pd.DataFrame({'class_fe2': bsample['class_fe2'].unique(),
                             'ag': random.choices([-1, 1], k = 12)})
  bsample = pd.merge(left = bsample, right = clusters, how = 'left',
                       on = 'class_fe2')
  bsample['berrors'] = bsample['eps_tilde'] * bsample['ag']
  bsample['beconmajor'] = (LPM_r.params['Constant'] + 
                             LPM_r.params['yr_2016'] * bsample['yr_2016'] +
                             LPM_r.params['treatment_class'] * bsample['treatment_class'] +
                             LPM_r.params['female_prof'] * bsample['female_prof'] +
                             LPM_r.params['instate'] * bsample['instate'] +
                             LPM_r.params['freshman'] * bsample['freshman'] +
                             LPM_r.params['american'] * bsample['american'] +
                             LPM_r.params['ACumGPA'] * bsample['ACumGPA'] +
                             LPM_r.params['gradePrinciples'] * bsample['gradePrinciples'] +
                             LPM_r.params['small_class'] * bsample['small_class'] +
                             bsample['berrors'])
  # Estimate artificial model
  LPM_b = Regpyhdfe(df = bsample, target = 'beconmajor',
                      predictors = ['yr_2016', 'treatment_class', 'female_prof', 
                                    'instate', 'freshman', 'american', 'ACumGPA', 
                                    'gradePrinciples', 'small_class', 'Constant',
                                    'treat2016'],
                      cluster_ids = ['class_fe2']).fit()
  # Store values
  WildClusterBootstrap.loc[b,'beta3']    = LPM_b.params['treat2016']
  WildClusterBootstrap.loc[b,'se_beta3'] = LPM_b.bse['treat2016']
  WildClusterBootstrap.loc[b,'t_stat']   = LPM_b.tvalues['treat2016']
```

We can see below what this "null distribution" of t-statistics looks like.  It is not a surprise that these are centred around 0, because this is what our model has imposed.  However, more interesting than this is to see they type of variation in t-statistics which we can expect in our data with null effects imposed.  We can see, below, that this looks somewhat heavier-tailed than a standard t-distribution.

```{python}
WildClusterBootstrap.plot(kind = 'hist', column = 't_stat', bins = 20,
                          title = '', legend = '', ylabel = '',
                          xlabel = 'Bootstrap t-statistics')
```

From this distribution we can calculate a p-value by asking what proportion of t-statistics from the null distribution exceed our estimated t-statistic from the unrestricted model.  We do this below, observing that the p-value is quite close to that reported in @PorterSerra2020 (who report a p-value of 0.089), only differing due to random variation in draws of the Rademacher weights.


<!--
Now we compare our original $t$-statistic `r round(t_beta3_hat, 3)` with the 97.5 percentile of the $t$-statistics computed in our bootstraping to see if we can reject the null of $\beta_3 = 0$ to a 5% significance level

{r}
t97_5 <- quantile(WildClusterBootstrap$t_stat, 0.975)
if(abs(t_beta3_hat) > abs(t97_5)){
  print("The null is rejected.")
} else {
  print("We can't reject the null.")
}

-->

```{python}
pval = (abs(WildClusterBootstrap['t_stat']) > abs(t_beta3_hat)).mean()
print('The p-value is: ', round(pval, 3))
```

We also could repeat this exercise with the `wildboottest` function from the `wildboottest` developed by @FischerRoodman2021 and arrive to the same conclusion.  This function works with the original model we estimated previously (`LPM`), and conducts an identical procedure to that which we have done above "by hand".  Any difference in p-values is incidental, owing to different random draws.  

```{python}
import statsmodels.formula.api as sm
model = sm.ols(data = data[data['female'] == 1].reset_index(), 
               formula = 'econmajor ~ yr_2016 + treatment_class + treat2016' +
               ' + female_prof + instate + freshman + american + ACumGPA +' +
               ' gradePrinciples + small_class')
from wildboottest.wildboottest import wildboottest
boot = wildboottest(model, param = 'treat2016', B = 999,
                     cluster = data[data['female'] == 1].reset_index().class_fe2)
print('The p-value with the user-written function is: ' +
      str(round(boot['p-value'].iloc[0], 3)))
```

In principle, using such a library is likely the preferred way of conducting procedures such as the wild cluster bootstrap, however it is illustrative to see how it works in practice, as we do above.  Although other feautres such as confidence intervals formed by inverting the test and iteratively searching for bounds are not available yet.

<!--
nice element of user-written procedures such as that of @Roodmanetal2019 is that it also seamlessly returns other quantities of interest which we would have to generate ourselves above, such as confidence interval, which we can see below:

{r}
paste0("The confidence interval is: [", 
       round(boot$conf_int[1], 3),
       ",", round(boot$conf_int[2], 3), "].")
These values correspond closely to the original 95% CIs reported in the paper of [-0.015; 0.160].
-->

Finally as a comparative exercise we may be interested in seeing how this procedure compares to a standard clustered bootstrap.  We do this by hand.  It turns out that while the 95% CI on `treat2016` coming from clustered bootstrap is narrower than the 95% CI from the wild cluster bootstrap (as expected), the difference is not *so* substantial in this particular case. 


```{python}
ClusterBootstrap = []
for _ in range(10000):
    sampled_clusters = random.choices(clusters.class_fe2, k = 12)
    bootstrap_sample = pd.concat([data[(data['class_fe2'] == cluster) & (data['female'] == 1)] for cluster in sampled_clusters])
    model = sm.ols(data = bootstrap_sample, 
                   formula = 'econmajor ~ yr_2016 + treatment_class + treat2016' +
                   ' + female_prof + instate + freshman + american + ACumGPA +' +
                   ' gradePrinciples + small_class').fit()
    ClusterBootstrap.append(model.params['treat2016'])
from numpy import percentile
percentile(a = ClusterBootstrap, q = [2.5, 97.5])
```

## Code Call Out 4.2: Exploring the Two-way Fixed Effect Model and Parameter Decomposition

**Two-Way Fixed Effects Estimators and Heterogeneous Treatment Effects** To understand the potential issues related to heterogeneous treatment effects over time and two-way fixed effect estimators, we will examine a pair of numerical examples. In particular, we will focus on the composition of the two way FE estimator $\tau$ estimated from: $$
y_{st} = \gamma_s + \lambda_t + \tau w_{st} + \varepsilon_{st}
$$ {#eq-twfe} where $y_{st}$ is the outcome variable, $\gamma_s$ and $\lambda_t$ are state (unit) and time fixed effects, $w_{st}$ is the binary treatment variable that takes the value of 1 if a state (unit) $s$ is treated at time $t$ and otherwise takes 0. We will work with a quite tractable example based on three units and 10 time periods, and will document how the approaches taken by @GoodmanBacon2018 and by @deChaisemartinDhaultfoeuille2019 to understand the two-way FE estimator compare.

The results from @GoodmanBacon2018 and those from @deChaisemartinDhaultfoeuille2019 are similar, however they take quite different paths to get there. Goodman-Bacon's (like that laid out in @AtheyImbens2018) is "mechanical" in that it is based on the underlying difference-in-differences comparisons between all groups. The result in @deChaisemartinDhaultfoeuille2019 is based on a potential outcomes frame-work, and counterfactuals under parallel trend assumptions. Thus to examine how these methods work requires somewhat different frameworks. In the case of @GoodmanBacon2018, we should consider all possible DD comparisons, while in the case of @deChaisemartinDhaultfoeuille2019 we should consider the treatment effect for each unit and time period, which requires knowing the observed and counterfactual state. While the approaches the two papers take to understand the content of the estimator differ, they refer to the same estimator, so always recover the same parameter estimate. To examine this in a more applied way, we will look at a simulated example.

To do this, let's consider a panel of 3 states/areas over the 10 years ($t$) of 2000 to 2009. One of these units is entirely untreated ($unit = 1$ or group $U$), one is treated at an early time period, 2003, ($unit = 2$ or group $k$), and the other is treated at a later time period, 2006, ($unit = 3$ or group $l$). We will construct a general structure for this data below:

```{python}
import pandas as pd
import numpy as np

Data = pd.DataFrame({'unit': np.ceil(np.arange(1,31)/10), 
                     'year': np.tile(np.arange(2000, 2010), 3)})
Data.head()
```

We will consider a simple-case where the actual data-generating process is known as: $$y_{unit,t} = 2 + 0.2 \times (t - 2000) + 1 \times unit + \beta_1 \times post \times unit + \beta_2 \times post \times unit \times (t - treat).$$ Here $unit$ refers to the unit number listed above (1, 2 or 3), $post$ indicates that a unit is receiving treatment in the relevant time period $t$, and $treat$ refers to the treatment period (2003 for unit 2, and 2006 for unit 3). Let's generate treatment, time to treatment, and post-treatment variables in `R`:

```{python}
Data['treat'] = np.where(Data['unit'] == 2, 2006, 
                         np.where(Data['unit'] == 3, 2003, 0))
Data['time'] = np.where(Data['treat'] == 0, 0, Data['year'] - Data['treat'])
Data['post'] = np.where(((Data['time'] >= 0) & (Data['treat'] != 0)), 1, 0) 
```

This specification allows for each unit to have its own fixed effect, given that $unit$ is multiplied by 1, and allows for a general time trend increasing by 0.2 units each period across the whole sample. These parameters are not so important, as what we care about are the treatment effects themselves. The impact of treatment comes from the units $\beta_1$ and $\beta_2$. The first of these, $\beta_1$, captures an immediate unit-specific jump when treatment is implemented which remains stable over time. The second of these, $\beta_2$, implies a trend break occurring *only* for the treated units once treatment comes into place. We will consider 2 cases below. In the first case $\beta_1 = 1$ and $\beta_2 = 0$ (a simple case with a constant treatment effect per unit):

```{python}
Data['y1'] = 2 + (Data['year'] - 2000) * 0.2 + 1 * Data['unit'] + 1 * Data['post'] * Data['unit'] + 0 * Data['post'] * Data['unit'] * (Data['time'])
```

and in a second case $\beta_1 = 1$ and $\beta_2 = 0.45$. This is a more complex case in which there are heterogeneous treatment effects over time:

```{python}
Data['y2'] = 2 + (Data['year'] - 2000) * 0.2 + 1 * Data['unit'] + 1 * Data['post'] * Data['unit'] + 0.45 * Data['post'] * Data['unit'] * (Data['time'])
```

These two cases are plotted next where the line with empty circles refers to group $U$, the line with black filled circles refers to group $k$ and the line with squares refers to group $l$

```{python fig.width=10}
#| code-fold: true
#| code-summary: "Show the plot code"
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme()
[Fig1, ax] = plt.subplots(1,2)
PanelA = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', size=1, ax = ax[0]) 
PanelA = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', marker="$\circ$", ax = ax[0])
PanelA = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', size=1, ax = ax[0])
PanelA = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', ax = ax[0])
PanelA = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', size=1, ax = ax[0])
PanelA = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', marker='s', ax = ax[0])
PanelA.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelA.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelA.set_yticks([0, 2, 4, 6, 8, 10, 12])
PanelA.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelA.legend([],[], frameon=False)
PanelA.set_ylabel('Outcome Variable')
PanelA.set_xlabel('Time')
PanelA.set_title('(a) Simple Decomposition')
PanelB = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', size=1, ax = ax[1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', marker="$\circ$", ax = ax[1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', size=1, ax = ax[1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', ax = ax[1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', size=1, ax = ax[1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', marker='s', ax = ax[1])
PanelB.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelB.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelB.set_yticks([0, 5, 10, 15, 20])
PanelB.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelB.legend([],[], frameon=False)
PanelB.set_ylabel('Outcome Variable')
PanelB.set_xlabel('Time')
PanelB.set_title('(b) Decomposition with trends')
```

### The Two-way Fixed Effect Estimator

First we will estimate the parameter by two-way fixed effects regression. This will provide us with the parameter estimate that both @GoodmanBacon2018 and @deChaisemartinDhaultfoeuille2019 will construct in a piece-wise fashion. This is done relatively simply in `R`. We simply estimate @eq-twfe by linear regression using `lm` as laid out below:

```{python}
import statsmodels.api as sm

case1 = sm.OLS.from_formula('y1 ~ post + C(unit) + C(year)', data=Data).fit()
print("The parameter estimates by two-way fixed effects regression for the case 1 is: ", case1.params["post"])
case2 = sm.OLS.from_formula('y2 ~ post + C(unit) + C(year)', data=Data).fit()
print("The parameter estimates by two-way fixed effects regression for the case 2 is: ", case2.params["post"])
```

Here we see that the coefficient of interest is 2.454545. We can see that this is between the two unit-specific jumps that occur with treatment (2 and 3). We will see below why it takes this particular weighted average.

### @GoodmanBacon2018 Decomposition

Using the values simulated above, let's see how the @GoodmanBacon2018 decomposition allows us to understand estimated treatment effects. We will consider both:\
- (a) Simple Decomposition\
- (b) Decomposition with trends

The methodology @GoodmanBacon2018 decomposition suggests that we should calculate all $2 \times 2$ combinations of states and time where post-treatment units are compared to "untreated" unit (laid out at more length in the boo). In this example, this provides four specific effects, which contribute to $\widehat{\tau}$ as a weighted mean. The specific effects desired are:

-   A. $\widehat{\beta}^{2\times2}_{kU}$ from the comparison of the early treated unit with the untreated unit.\
-   B. $\widehat{\beta}^{2\times2}_{lU}$, from the comparison of the latter treated unit with the untreated unit.\
-   C. $\widehat{\beta}^{2\times2,k}_{kl}$, from the comparison of the early and latter treated units, when the early unit begin to be treated.\
-   D. $\widehat{\beta}^{2\times2,l}_{kl}$, from the comparison of the early and latter treated units, when the latter unit begin to be treated.

These will then be weighted as laid out in @GoodmanBacon2018 to provide the regression-based estimate.

#### (a) Simple Decomposition

In this case the @GoodmanBacon2018 methodology estimate $\widehat{\tau}$ weighting the next four DD comparisons

```{python}
#| code-fold: true
#| code-summary: "Show the plot code"
[Fig2, ax] = plt.subplots(2,2)
### PanelA
PanelA = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,0]) 
PanelA = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', marker="$\circ$", ax = ax[0,0])
PanelA = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,0], alpha = 0.1)
PanelA = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', ax = ax[0,0], alpha = 0.1)
PanelA = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,0])
PanelA = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', marker='s', ax = ax[0,0])
PanelA.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelA.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelA.set_yticks([0, 2, 4, 6, 8, 10, 12])
PanelA.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelA.legend([],[], frameon=False)
PanelA.set_ylabel('Outcome Variable')
PanelA.set_xlabel('Time')
PanelA.set_title('A. Early Group v/s Untreated Group')
### PanelB
PanelB = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y1',
                      color='black', marker="$\circ$", ax = ax[0,1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y1',
                      color='black', ax = ax[0,1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', size=1, ax = ax[0,1], alpha = 0.1)
PanelB = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y1',
                      color='black', marker='s', ax = ax[0,1], alpha = 0.1)
PanelB.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelB.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelB.set_yticks([0, 2, 4, 6, 8, 10, 12])
PanelB.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelB.legend([],[], frameon=False)
PanelB.set_ylabel('Outcome Variable')
PanelB.set_xlabel('Time')
PanelB.set_title('B. Later Group v/s Untreated Group')
### PanelC
PanelC = sns.lineplot(data=Data[(Data['unit'] == 1) & (Data['year'] < 2006)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,0], 
                      alpha = 0.1) 
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 1) & (Data['year'] < 2006)], 
                         x ='year', y='y1', color='black', marker="$\circ$", 
                         ax = ax[1,0], alpha = 0.1)
PanelC = sns.lineplot(data=Data[(Data['unit'] == 2) & (Data['year'] < 2006)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,0])
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 2) & (Data['year'] < 2006)], 
                         x ='year', y='y1', color='black', ax = ax[1,0])
PanelC = sns.lineplot(data=Data[(Data['unit'] == 3) & (Data['year'] < 2006)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,0])
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 3) & (Data['year'] < 2006)], 
                         x ='year', y='y1', color='black', marker='s', 
                         ax = ax[1,0])
PanelC.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelC.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelC.set_yticks([0, 2, 4, 6, 8, 10, 12])
PanelC.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelC.legend([],[], frameon=False)
PanelC.set_ylabel('Outcome Variable')
PanelC.set_xlabel('Time')
PanelC.set_title('C. Early Group v/s Later Group Before 2006')
### PanelD
PanelD = sns.lineplot(data=Data[(Data['unit'] == 1) & (Data['year'] > 2002)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,1], 
                      alpha = 0.1)
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 1) & (Data['year'] > 2002)], 
                         x ='year', y='y1', color='black', marker="$\circ$", 
                         ax = ax[1,1], alpha = 0.1)
PanelD = sns.lineplot(data=Data[(Data['unit'] == 2) & (Data['year'] > 2002)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,1])
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 2) & (Data['year'] > 2002)], 
                         x ='year', y='y1', color='black', ax = ax[1,1])
PanelD = sns.lineplot(data=Data[(Data['unit'] == 3) & (Data['year'] > 2002)], 
                      x ='year', y='y1', color='black', size=1, ax = ax[1,1])
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 3) & (Data['year'] > 2002)], 
                         x ='year', y='y1', color='black', marker='s', 
                         ax = ax[1,1])
PanelD.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelD.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelD.set_yticks([0, 2, 4, 6, 8, 10, 12])
PanelD.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelD.legend([],[], frameon=False)
PanelD.set_ylabel('Outcome Variable')
PanelD.set_xlabel('Time')
PanelD.set_title('D. Early Group v/s Later Group After 2003')
```

As seen in the plots, in the simple decomposition these effects are constants of 3 and 2 for early and later treated units given that the "treatment effect" is simply $1 \times unit$ in each case.

##### A. Early Group v/s Untreated Group

In order to calculate the effects we start making the simple DD comparison of the untreated group $U$ ($unit = 1$) with the early treated group $k$ ($unit = 3$) getting $\widehat{\beta}^{2 \times 2}_{kU}$ as $$\widehat{\beta}^{2 \times 2}_{kU} = \left( \overline{y}_k^{Post(k)} - \overline{y}_k^{Pre(k)} \right) - \left( \overline{y}_U^{Post(k)} - \overline{y}_U^{Pre(k)} \right)$$ Where $\overline{y}_k^{Post(k)}$ is the mean of the outcome variable for the early treated group $k$ ($unit = 3$) posterior to treatment, from 2003, $\overline{y}_k^{Pre(k)}$ is the mean for of the outcome variable for the early treated group $U$ ($unit = 3$) prior to treatment, (up until 2002), and $\overline{y}_U^{Post(k)}, \overline{y}_U^{Post(k)}$ are the analogous quantities for the untreated group $U$ ($unit = 1$)

```{python}
((Data[(Data['unit'] == 3) & (Data['post'] == 1)]['y1'].mean() - 
 Data[(Data['unit'] == 3) & (Data['post'] == 0)]['y1'].mean()) -
(Data[(Data['unit'] == 1) & (Data['year'] >= 2003)]['y1'].mean() - 
 Data[(Data['unit'] == 1) & (Data['year'] < 2003)]['y1'].mean()))
```

This result also can be obtained from the linear regression with the canonical DD formula $$y_{unit,t} = \alpha_0 + \alpha_1 \times Post(k) + \alpha_2 \times \mathbf{1}(unit = 3) + \beta_{kU}^{2\times2} \times Post(k) \times \mathbf{1}(unit = 3) + \varepsilon_i$$ Where $Post(k)$ indicates that the year is equal or greater than the year where the group $k$ ($unit = 3$) received the treatment (2003) and $\mathbf{1}(unit = 3)$ indicates if the observation is from the early treated group $k$ ($unit = 3$)

```{python}
Data['post2003'] = np.where(Data['year'] >= 2003, 1, 0)
sm.OLS.from_formula('y1 ~ C(post2003) + C(unit) + C(post2003):C(unit)', 
                    data=Data[Data['unit'] != 2]).fit().summary()
```

A third way to obtain this is from the next linear regression $$y_{unit,t} = \alpha_0 + \beta_{kU}^{2 \times 2} \times Post + \sum_{i = 2001}^{2009} \alpha_{i-2000} \times \mathbf{1}(year = i) + \alpha_{10} \times \mathbf{1}(unit = 3) + \varepsilon_i$$ Where in this case $Post$ indicates if the unit is treated (note for group $U$ this will be always 0), $\mathbf{1}(year = i)$ indicates if the observation is in period $i \in \{2001, \ldots, 2009\}$ and $\mathbf{1}(unit = 3)$ keep its meaning

```{python}
sm.OLS.from_formula('y1 ~ post  + C(year) + C(unit)', 
                    data=Data[Data['unit'] != 2]).fit().summary()
```

Now we store this result for posterior use

```{python}
bku = sm.OLS.from_formula('y1 ~ post  + C(year) + C(unit)', 
                    data=Data[Data['unit'] != 2]).fit().params['post']
```

##### B. Later Group v/s Untreated Group

The next DD comparison we calculate is that which compares the later treated group $l$ ($unit = 2$) with the untreated group $U$ ($unit = 1$), resulting in $\widehat{\beta}^{2 \times 2}_{lU}$. As above, we can generate this DD estimate in a number of ways (most simply by double-differencing with means), and this will then be stored.

```{python}
blu = sm.OLS.from_formula('y1 ~ post  + C(year) + C(unit)', 
                    data=Data[Data['unit'] != 3]).fit().params['post']
print(blu)
print((Data[(Data['unit'] == 2) & (Data['post'] == 1)]['y1'].mean() - 
 Data[(Data['unit'] == 2) & (Data['post'] == 0)]['y1'].mean()) -
(Data[(Data['unit'] == 1) & (Data['year'] >= 2006)]['y1'].mean() - 
 Data[(Data['unit'] == 1) & (Data['year'] < 2006)]['y1'].mean()))
Data['post2006'] = np.where(Data['year'] >= 2006, 1, 0)
sm.OLS.from_formula('y1 ~ C(post2006) + C(unit) + C(post2006):C(unit)', 
                    data=Data[Data['unit'] != 3]).fit().summary()
```

##### C. Early Group v/s Later Group Before 2006

Next we calculate the effects from the DD comparisons of early and later treated groups, up until the later treated group receives treatment (2006). This is: $$\widehat{\beta}^{2 \times 2, k}_{kl} \equiv \left( \overline{y}^{Mid(k,l)}_{k} - \overline{y}^{Pre(k)}_{k} \right) - \left( \overline{y}^{Mid(k,l)}_{l} - \overline{y}^{Pre(k)}_{l} \right)$$ where $\overline{y}^{Mid(k,l)}_{k}$ is the mean of the outcome variable for the early treated group $k$ ($unit = 3$) in the period between the treatment for the group $k$ and the group $l$ ($unit = 2$), from 2003 to 2005, $\overline{y}^{Pre(k)}_{k}$ is the mean for of the outcome variable for the early treated group $k$ ($unit = 3$) previous to treatment, until 2002, and $\overline{y}^{Mid(k,l)}_{l}, \overline{y}^{Pre(k)}_{l}$ are the analogous for the later treated group $l$ ($unit = 2$)

```{python}
bklk = sm.OLS.from_formula('y1 ~ post  + C(year) + C(unit)', 
                    data=Data[(Data['unit'] != 1) & (Data['year'] < 2006)]).fit().params['post']
print(bklk)
print((Data[(Data['unit'] == 3) & ((Data['year'] >= 2003) & (Data['year'] < 2006))]['y1'].mean() - 
 Data[(Data['unit'] == 3) & (Data['year'] < 2003)]['y1'].mean()) -
(Data[(Data['unit'] == 2) & ((Data['year'] >= 2003) & (Data['year'] < 2006))]['y1'].mean() - 
 Data[(Data['unit'] == 2) & (Data['year'] < 2003)]['y1'].mean()))
sm.OLS.from_formula('y1 ~ C(post2003) + C(unit) + C(post2003):C(unit)', 
                    data=Data[(Data['unit'] != 1) & (Data['year'] < 2006)]).fit().summary()
```

##### D. Early Group v/s Later Group After 2003

The last DD comparison is for early and later treated groups, starting from 2006 $$\widehat{\beta}^{2 \times 2, l}_{kl} \equiv \left( \overline{y}^{Post(l)}_{l} - \overline{y}^{Mid(k,l)}_{l} \right) - \left( \overline{y}^{Post(l)}_{k} - \overline{y}^{Mid(k,l)}_{k} \right)$$ Where $\overline{y}^{Post(l)}_{l}$ is the mean of the outcome variable for the later treated group $l$ ($unit = 2$) in the period after this group received the treatment, from 2006, $\overline{y}^{Mid(k,l)}_{l}$ is the mean for of the outcome variable for the later treated group $l$ ($unit = 2$) in the period between the treatment for the group $k$ ($unit = 3$) and the group $l$, from 2003 to 2005, and $\overline{y}^{Post(l)}_{k}, \overline{y}^{Mid(k,l)}_{k}$ are the analogous quantities for the early treated group $k$ ($unit = 3$). We can generate and save this quantity as we have previously:

```{python}
bkll = sm.OLS.from_formula('y1 ~ post  + C(year) + C(unit)', 
                    data=Data[(Data['unit'] != 1) & (Data['year'] > 2002)]).fit().params['post']
print(bkll)
print((Data[(Data['unit'] == 2) & (Data['year'] > 2005)]['y1'].mean() - 
 Data[(Data['unit'] == 2) & ((Data['year'] >= 2003) & (Data['year'] < 2006))]['y1'].mean()) -
(Data[(Data['unit'] == 3) & (Data['year'] > 2005)]['y1'].mean() - 
 Data[(Data['unit'] == 3) & ((Data['year'] >= 2003) & (Data['year'] < 2006))]['y1'].mean()))
sm.OLS.from_formula('y1 ~ C(post2006) + C(unit) + C(post2006):C(unit)', 
                    data=Data[(Data['unit'] != 1) & (Data['year'] > 2002)]).fit().summary()
```

This comparison is the comparison which can potentially result in undesired results if treatment effects are dynamic over time because it views group 3 (the previously treated group) as a control. However, in this case, given that treatment effects are homogenous over time we do not have a major problem here, and we observe that $\widehat{\beta}^{2 \times 2, l}_{kl}=2$.

##### Weights

We can now arrive to the OLS estimate of this two-way fixed effect model by generating the weighted mean of the previous estimates as: $$\widehat{\tau} = W_{kU} \cdot \widehat{\beta}^{2\times 2}_{kU} + W_{lU} \cdot \widehat{\beta}^{2\times 2}_{lU} + W_{kl}^{k} \cdot \widehat{\beta}^{2\times 2,k}_{kl} + W_{kl}^{l} \cdot \widehat{\beta}^{2\times 2,l}_{kl}$$ Where each $W$ is the weight that the respective $\beta$ has in this weighted mean, specifically: \begin{align*} 
W_{kU} & = \frac{(n_k + n_U)^2\widehat{V}^D_{kU}}{\widehat{V}^D} \quad &  \quad W_{lU} & = \frac{(n_l + n_U)^2\widehat{V}^D_{lU}}{\widehat{V}^D} \\ 
W_{kl}^k & = \frac{[(n_k + n_l)(1 - \overline{D}_l)]^2\widehat{V}^{D,k}_{kl}}{\widehat{V}^D} \quad &  \quad W_{kl}^l & = \frac{[(n_k + n_l)(1 - \overline{D}_k)]^2\widehat{V}^{D,l}_{kl}}{\widehat{V}^D}
\end{align*} Where $n$ refers to the sample share of the group

```{python}
nk = 1/3
nl = 1/3
nu = 1/3
```

$\overline{D}$ referes to the share of time the group is treated

```{python}
Dk = Data[Data['unit'] == 3]['post'].mean()
Dl = Data[Data['unit'] == 2]['post'].mean()
```

and $\widehat{V}$ refers to how much treatment varies

```{python}
VkU = 0.5*0.5*(Dk)*(1-Dk)
VlU = 0.5*0.5*(Dl)*(1-Dl) 
Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
VD   = (sm.OLS.from_formula('post ~ C(unit) + C(year)', 
                           data=Data).fit().resid**2).mean()
```

The weights are thus the following:

```{python}
wkU = (((nk + nu)**2)*VkU)/VD
print(wkU)
wlU = (((nl + nu)**2)*VlU)/VD
print(wlU)
wklk = ((((nk + nl)*(1-Dl))**2)*Vklk)/VD
print(wklk)
wkll = ((((nk + nl)*Dk)**2)*Vkll)/VD
print(wkll)
```

With this in mind the $\tau$ estimate is

```{python}
tau = wkU * bku + wlU * blu + wklk * bklk + wkll * bkll
print(tau)
```

as observed in the two-way fixed effect estimate above.

#### (b) Decomposition with trends

In this case the @GoodmanBacon2018 decomposition follows as above generating the treatment effect as follows:

```{python}
#| code-fold: true
#| code-summary: "Show the plot code"
[Fig3, ax] = plt.subplots(2,2)
### PanelA
PanelA = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,0]) 
PanelA = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', marker="$\circ$", ax = ax[0,0])
PanelA = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,0], alpha = 0.1)
PanelA = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', ax = ax[0,0], alpha = 0.1)
PanelA = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,0])
PanelA = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', marker='s', ax = ax[0,0])
PanelA.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelA.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelA.set_yticks([0, 5, 10, 15, 20])
PanelA.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelA.legend([],[], frameon=False)
PanelA.set_ylabel('Outcome Variable')
PanelA.set_xlabel('Time')
PanelA.set_title('A. Early Group v/s Untreated Group')
### PanelB
PanelB = sns.lineplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 1], x ='year', y='y2',
                      color='black', marker="$\circ$", ax = ax[0,1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,1])
PanelB = sns.scatterplot(data=Data[Data['unit'] == 2], x ='year', y='y2',
                      color='black', ax = ax[0,1])
PanelB = sns.lineplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', size=1, ax = ax[0,1], alpha = 0.1)
PanelB = sns.scatterplot(data=Data[Data['unit'] == 3], x ='year', y='y2',
                      color='black', marker='s', ax = ax[0,1], alpha = 0.1)
PanelB.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelB.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelB.set_yticks([0, 5, 10, 15, 20])
PanelB.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelB.legend([],[], frameon=False)
PanelB.set_ylabel('Outcome Variable')
PanelB.set_xlabel('Time')
PanelB.set_title('B. Later Group v/s Untreated Group')
### PanelC
PanelC = sns.lineplot(data=Data[(Data['unit'] == 1) & (Data['year'] < 2006)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,0], 
                      alpha = 0.1) 
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 1) & (Data['year'] < 2006)], 
                         x ='year', y='y2', color='black', marker="$\circ$", 
                         ax = ax[1,0], alpha = 0.1)
PanelC = sns.lineplot(data=Data[(Data['unit'] == 2) & (Data['year'] < 2006)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,0])
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 2) & (Data['year'] < 2006)], 
                         x ='year', y='y2', color='black', ax = ax[1,0])
PanelC = sns.lineplot(data=Data[(Data['unit'] == 3) & (Data['year'] < 2006)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,0])
PanelC = sns.scatterplot(data=Data[(Data['unit'] == 3) & (Data['year'] < 2006)], 
                         x ='year', y='y2', color='black', marker='s', 
                         ax = ax[1,0])
PanelC.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelC.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelC.set_yticks([0, 5, 10, 15, 20])
PanelC.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelC.legend([],[], frameon=False)
PanelC.set_ylabel('Outcome Variable')
PanelC.set_xlabel('Time')
PanelC.set_title('C. Early Group v/s Later Group Before 2006')
### PanelD
PanelD = sns.lineplot(data=Data[(Data['unit'] == 1) & (Data['year'] > 2002)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,1], 
                      alpha = 0.1)
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 1) & (Data['year'] > 2002)], 
                         x ='year', y='y2', color='black', marker="$\circ$", 
                         ax = ax[1,1], alpha = 0.1)
PanelD = sns.lineplot(data=Data[(Data['unit'] == 2) & (Data['year'] > 2002)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,1])
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 2) & (Data['year'] > 2002)], 
                         x ='year', y='y2', color='black', ax = ax[1,1])
PanelD = sns.lineplot(data=Data[(Data['unit'] == 3) & (Data['year'] > 2002)], 
                      x ='year', y='y2', color='black', size=1, ax = ax[1,1])
PanelD = sns.scatterplot(data=Data[(Data['unit'] == 3) & (Data['year'] > 2002)], 
                         x ='year', y='y2', color='black', marker='s', 
                         ax = ax[1,1])
PanelD.axvline(2002, color = 'red', linestyle='dashed', linewidth=1)
PanelD.axvline(2005, color = 'red', linestyle='dashed', linewidth=1)
PanelD.set_yticks([0, 5, 10, 15, 20])
PanelD.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelD.legend([],[], frameon=False)
PanelD.set_ylabel('Outcome Variable')
PanelD.set_xlabel('Time')
PanelD.set_title('D. Early Group v/s Later Group After 2003')
```

As seen in the plots, in the decomposition with trends these effects are no longer constants of 3 and 2 for early and later treated units given that the "treatment effect" is no longer simply $1 \times unit$ in each case.

```{python}
# 2X2 DD Regressions
panelA = sm.OLS.from_formula('y2 ~ post + C(year) + C(unit)', 
                         data=Data[Data['unit'] != 2]).fit()
panelB = sm.OLS.from_formula('y2 ~ post + C(year) + C(unit)', 
                         data=Data[Data['unit'] != 3]).fit()
panelC = sm.OLS.from_formula('y2 ~ post + C(year) + C(unit)', 
                         data=Data[(Data['unit'] != 1) & (Data['year'] < 2006)]).fit()
panelD = sm.OLS.from_formula('y2 ~ post + C(year) + C(unit)', 
                         data=Data[(Data['unit'] != 1) & (Data['year'] > 2002)]).fit()
# 2x2 Betas
bkUk = panelA.params["post"]
bkUl = panelB.params["post"]
bklk = panelC.params["post"]
bkll = panelD.params["post"]
# Share of time treated
Dk = Data[Data['unit'] == 3]['post'].mean()
Dl = Data[Data['unit'] == 2]['post'].mean()
# How much treatment varies
VkU = 0.5*0.5*(Dk)*(1-Dk)
VlU = 0.5*0.5*(Dl)*(1-Dl) 
Vklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))
Vkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))
VD   = (sm.OLS.from_formula('post ~ C(unit) + C(year)', 
                           data=Data).fit().resid**2).mean()
# Share of sample
nk   = 1/3
nl   = 1/3
nu   = 1/3
# Weights
wkUk = (((nk + nu)**2)*VkU)/VD
wkUl = (((nl + nu)**2)*VlU)/VD
wklk = ((((nk + nl)*(1-Dl))**2)*Vklk)/VD
wkll = ((((nk + nl)*Dk)**2)*Vkll)/VD
# Tau
tau = bkUk*wkUk + bkUl*wkUl + bklk*wklk + bkll*wkll
print(tau)
```

What is noteworthy here is the surprising behaviour flagged by @GoodmanBacon2018 for the final comparison based on the case where the earlier treated unit (unit 3) is used as a control for the later trated unit (unit 2). In this case, given that there *are* time-varying treatment effects, despite the fact that each unit-specific treatment effect is positive, we observe that the parameter $\widehat{\beta}^{2 \times 2, l}_{kl}$ is actually *negative*. In this particular example this negative value (-1.375) is not sufficient to turn the weighted treatment effect estimate negative, but if you play around with the size of the parameters $\beta_1$ and $\beta_2$ above, you will see that large enough differences in trends *can* result in such estimates! Here, as above, we see that when we aggregate unit-specific estimates as `tau`, the estimate (by definition) agrees with the estimate generated by two-way fixed effect models previously.

### @deChaisemartinDhaultfoeuille2019's Procedure

Now, we will show that the procedures described in @deChaisemartinDhaultfoeuille2019, despite arriving to the estimator in a different way, also let us understand how the regression weights the two-way fixed effect estimator. In this case, rather than considering each treatment-control comparison pair, the authors note that the two-way fixed estimator can be conceived as a weighted sum of each single group by time period in any post-treatment group.

The authors define $\widehat{\beta}_{fe}$ as the coefficient estimated in the following (standard) two-way fixed effects regression: $$y_{i,s,t} = \beta_0 + \beta_{fe} D_{s,t} + \mu_s + \lambda_t + \varepsilon_{s,t}$$ Where $D_{s,t}$ is the mean over $i$ of a binary indicator variable that takes value of 1 if the unit $i$ in state $s$ is treated at period $t$ and 0 otherwise, in our case as we have one observartion per state $D_{s,t} = post_{s,t}$, meanwhile $\mu_s$ and $\lambda_t$ are state and time fixed effects. This is, of course, precisely the same model as we have estimated in @eq-twfe, implying that $\beta_{fe}=2.4545$ in cases without post-treatment trends (`y1`), or $\beta_{fe}=3.8045$ in cases with post-treatment dynamics (`y2`).

@deChaisemartinDhaultfoeuille2019 define the ATE for any ($s,t$) cell as: $$\Delta_{s,t} = \frac{1}{N_{s,t}} \sum_{i = 1}^{N_{s,t}}[Y_{i,s,t}(1) - Y_{i,s,t}(0)].$$  You will note that here we require an unobserved counterfactual $Y_{i,s,t}(0)$.  If we impose a parallel trend assumption, such a counterfactual can be inferred from unit-specific fixed effects, time-specific fixed effects, and the constant term.  Because in this case we *know* our data generating process, we can simply generate this counterfactual as the data generating process, absent any effect of treatment.  Below we generate such a counterfactual, where you will note that we impose that this is an 'untreated' counterfactual by setting the treatment effects to 0 in the generation of `y1_c` below:
```{python}
Data['y1_c'] = 2 + (Data['year'] - 2000) * 0.2 + 1 * Data['unit'] + 0 * Data['post'] * Data['unit'] + 0 * Data['post'] * Data['unit'] * (Data['time'])
```

It is likely useful to confirm to ourselves that graphically we are indeed generating the untreated counterfactual in this way.

```{python}
#| code-fold: true
#| code-summary: "Show the plot code"
[Fig4, ax] = plt.subplots(1,2)
### Unit 2
PanelA = sns.lineplot(data=Data[Data['unit'] == 2], x = 'year', y = 'y1',
                      color = 'blue', size = 1, ax = ax[0])
PanelA = sns.lineplot(data = Data[Data['unit'] == 2], x = 'year', y = 'y1_c',
                      color = 'red', size = 1, ax = ax[0], linestyle = 'dashed')
PanelA.legend([],[],frameon=False)
PanelA.set_yticks([4, 5, 6, 7, 8])
PanelA.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelA.set_ylabel("Y")
PanelA.set_xlabel("Year")
PanelA.set_title("(a) Unit 2 Outcome and Countefactual", y = -0.25)
### Unit 3
PanelB = sns.lineplot(data=Data[Data['unit'] == 3], x = 'year', y = 'y1',
                      color = 'blue', size = 1, ax = ax[1])
PanelB = sns.lineplot(data = Data[Data['unit'] == 3], x = 'year', y = 'y1_c',
                      color = 'red', size = 1, ax = ax[1], linestyle = 'dashed')
PanelB.legend([],[],frameon=False)
PanelB.set_yticks([5, 6, 7, 8, 9, 10])
PanelB.set_xticks([2000, 2002, 2004, 2006, 2008])
PanelB.set_ylabel("Y")
PanelB.set_xlabel("Year")
PanelB.set_title("(b) Unit 3 Outcome and Countefactual", y = -0.25)
```

This allows us to calculate a state- and time-period specific treatment effect ($\Delta_{s,t}$) for each treated unit.  We do so, calculating this quantity for all units in which treatment exists:

```{python}
Data['Delta_st'] = np.where(Data['post'] == 1, Data['y1'] - Data['y1_c'],
                            np.nan)
Data[Data['post'] == 1][['y1', 'y1_c', 'unit', 'year', 'Delta_st']]
```

Unsurprisingly, given the data generating process we have defined, we see that each treatment effect is 2 for unit 2, and 3 for unit 3. If we were to calculate a mean treatment effect by hand, we may wish to simply take an average over all periods and units.  However, one of the key results of @deChaisemartinDhaultfoeuille2019 is to show that under a series of standard assumptions $$\beta_{fe} = E \left[ \sum_{s,t:D_{s,t}=1}\frac{N_{s,t}}{N_1}w_{s,t}\Delta_{s,t} \right]$$ Where $N_1$ refers to the sum of all treated observations and $$w_{s,t} = \frac{\varepsilon_{s,t}}{\sum_{s,t:D_{s,t}=1}\frac{N_{s,t}}{N_1}\varepsilon_{s,t}}$$ Where $\varepsilon_{s,t}$ is the residual from a regression of $D_{s,t}$ on state and time fixed-effects.  To confirm this in our data, we will estimate these regression residuals and add them into the dataframe:

```{python}
auxreg = sm.OLS.from_formula('post ~ C(year) + C(unit)',
                             data=Data).fit()
Data['eps_st'] = auxreg.resid
Data['eps_st'] = np.where(Data['post'] != 1, np.nan, Data['eps_st'])
Data['w_st'] = Data['eps_st'] / Data['eps_st'].sum()
print(round(Data[Data['post'] == 1][['y1', 'y1_c', 'unit', 'year', 'Delta_st', 'w_st']], 6))
```
Note here that after generating $w_{s,t}$ we print this out using the round function to avoid very small digits appearing which are only different to zero given machine precision.  The key thing that we can see is that the effective weighting of treatment effects which occurs in regression is quite different to what we would expect.  Indeed, four periods are given 0 weights!  Finally, we can confirm that this decomposition gives us the two-way fixed effect estimate by multiplying $\Delta_{s,t}$ and $w_{s,t}$ and summing:


```{python}
print("de Chaisemartin and Xavier D'Haultfoeuille's decomposition returns an estimates of: " + str((Data['Delta_st'] * Data['w_st']).sum()))
```
We can see that correctly, this decomposition also returns the two-way fixed effect estimate of 2.4545.


We can follow precisely the same series of steps to see the case of the decomposition where treatment exposition also results in a trend-break.  To see this, we conduct each of the above steps below, however here we have not produced similar graphs (though you may wish to do so to confirm that counterfactuals make sense):

```{python}
Data['y2_c'] = 2 + (Data['year'] - 2000) * 0.2 + 1 * Data['unit'] + 0 * Data['post'] * Data['unit'] + 0 * Data['post'] * Data['unit'] * (Data['time'])
Data['Delta_st2'] = np.where(Data['post'] == 1, Data['y2'] - Data['y2_c'],
                            np.nan)
print(round(Data[Data['post'] == 1][['y2', 'y2_c', 'unit', 'year', 'Delta_st', 'w_st']], 6))
```

Because there is no difference in the structure of the treatment indicator or the unit and time fixed effects, the residuals $w_{s,t}$ are identical, though of course the treatment effects themselves, $\Delta_{s,t}$ are not.  Thus, once again we see that later treatment effects for unit 3 (precisely those units for which treatment effects are largest), are given zero weights.  Finally, again we can calculate the two-way fixed effect estimate following this decomposition by summing across units, capturing the estimate we have previously observed in regression models of 3.804545.  

```{python}
print("de Chaisemartin and Xavier D'Haultfoeuille's decomposition returns an estimates of: " + str((Data['Delta_st2'] * Data['w_st']).sum()))
```

Depending on the nature of treatment assignment, ie the number of treated periods, as well as the period in which treatment is adopted in different units, these weights will vary, and can even be negative.  You may wish to explore alternative set-ups and confirm to yourself that this is the case, and see that regardless of the nature of the setting, both @GoodmanBacon2018 and @deChaisemartinDhaultfoeuille2019's decompositions recover the two-way fixed effect estimate.

## Code call-out 4.3: Event study and Interaction-weighted Estimators
To understand the equivalence between the panel event study model described in Section 4.4.2.1 of the book and the "Interaction-weighted (IW) estimator" proposed by @AbrahamSun2018 we work with data from @StevensonWolfers2006 which examines the effect of the staggered adoption of no-default divorce reforms (`_nfd`) and female suicide (`asmrs`) in United States for 49 states (`stfips`) from 1964 to 1996.  We begin by loading the data below, and confirming that it effectively consists of a balanced sample of 49 states (we will denote using $s$ below) over 33 years (denoted as $t$):

```{python}
import pandas as pd
data = pd.read_csv("Datasets/StevensonWolfers2006.csv")
print(len(data))
print(data.head())
```

In order to prepare our dataset we note that the variable `_nfd` contains the year a state adopts a law ($Event_s$), and define a variable `timeToTreat` as the difference between year $t$  and $Event_s$:

```{python}
data["timeToTreat"] = data["year"] - data["_nfd"]
data[["year", "_nfd", "timeToTreat"]].head(10)
```

Because `_nfd' is missing for states which did not pass a no fault divorce law in the period under study, this variable thus captures leads (periods prior to treatment) and lags (periods post treatment) for states which have adopted a no fault divorce law.

### Panel Event Study Model
We will begin by estimating a standard event study, defined as follows, or as equation XXXX in the book:
$$asmrs_{st} = \alpha + \sum_{j=2}^{J} \beta_j (Lead \ j)_{st} + \sum_{k = 0}^{K} \gamma_{k} (Lag \ k)_{st} + \mu_s + \lambda_t + X_{st}^\prime \Gamma + \varepsilon_{st}$$
Here $asmrs_{st}$ refers to the female suicide rate for all women of state $s$ at period $t$, $(Lead \ j)_{st}$ a dummy variable that takes 1 if the state $s$ at period $t$ is $j$ periods pre-treatment, $(Lag \ k)_{st}$ a dummy variable that takes 1 if the state $s$ at period $t$ is $k$ periods post-treatment, $\mu_s$ and $\lambda_t$ are state and time fixed effects respectively and $X^\prime_{st}$ a vector of covariates for state $s$ at period $t$ such as per-capita income $pcinc_{st}$, homicide mortality $asmrh_{st}$ and the aid to families with dependent children (AFDC) rate for a family of four $cases_{st}$. 

Thus, we wish to include a single binary variable for each lead and lag observed in our data (arbritarily omitting lead 1). If we inspect the values of `timeToTreat` below, we can see how there are $J = 21$ binary $Lead$ variables and $K = 27$ $Lag$ variable to include:

```{python}
data['timeToTreat'].unique()
```

A natural option to generate lags and leads is to use the `get_dummies` function from `pandas`library, but this don't generate clearly names so we write a function to rename the columns generated by the `get_dummies` function and then join it to the original data frame

```{python}
# Generate dummies
dummies_timeToTreat = pd.get_dummies(data['timeToTreat']).astype(int)

# Generate function to rename columins
def ren_cols(col):
    if col < 0:
        return 'Lead' + str(abs(int(col)))
    else:
        return 'Lag' + str(int(col))

# Rename the columns
dummies_timeToTreat.columns = [ren_cols(col) for col in dummies_timeToTreat.columns]

# Join to the original data frame
data = pd.concat([data, dummies_timeToTreat], axis = 1)
data[["year", "_nfd", "timeToTreat", "Lead7", "Lead6", "Lead5"]].head(5)
```

Next we can estimate the event study by standard OLS using the `Regpyhdfe` function from the `regpyhdfe` package.  Note that we omit `Lead1` as a reference base level.  The usage of this function includes several arguments where the `target` argument points the dependent variable, the `predictors` argument are the explanatory covaraites, the `absorb_ids` argument determine the fixed effects (in this case year and state fixed effects) and the `cluster_ids` argument is for the variable to use if clustered standard errors are desired.

```{python}
from regpyhdfe import Regpyhdfe
EventStudy = Regpyhdfe(df = data, target = 'asmrs', 
                       predictors = ['Lead21', 'Lead20', 'Lead19', 'Lead18',
                                     'Lead17', 'Lead16', 'Lead15', 'Lead14',
                                     'Lead13', 'Lead12', 'Lead11', 'Lead10',
                                     'Lead9', 'Lead8', 'Lead7', 'Lead6', 'Lead5',
                                     'Lead4', 'Lead3', 'Lead2', 'Lag0', 'Lag1',
                                     'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Lag6',
                                     'Lag7', 'Lag8', 'Lag9', 'Lag10', 'Lag11',
                                     'Lag12', 'Lag13', 'Lag14', 'Lag15', 'Lag16',
                                     'Lag17', 'Lag18', 'Lag19', 'Lag20', 'Lag21',
                                     'Lag22', 'Lag23', 'Lag24', 'Lag25', 'Lag26',
                                     'Lag27', 'pcinc', 'asmrh', 'cases'],
                       absorb_ids=['stfips', 'year'],
                       cluster_ids=['stfips']).fit()
print(EventStudy.summary2())
```

Once we have estimated this regression, we can visualise point estimates and standard errors in the traditional event study style, as laid out below.  To do this, we will create a data frame that incorporates the parameters we need from our regression, which we saved as `EventStudy` above.  Finally, we will plot an event study using `pyplot`:

```{python}
params = pd.DataFrame({'Estimate': EventStudy.params,
                       'Std. Err': EventStudy.bse})
params = params.drop(['pcinc', 'asmrh', 'cases'])
# Create df
plot_df = pd.concat([params.loc['Lead21':'Lead2'], 
                     pd.DataFrame({'Estimate': 0, 'Std. Err': 0},
                                  index=['Lead1']),
                     params.loc['Lag0':'Lag27']])
plot_df['Time'] = list(range(-21,28))
# Graph
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()
ES = plot_df.plot(x = 'Time', y = 'Estimate', yerr = 'Std. Err', kind='scatter',
                  marker='o', facecolor='none', s=25, color='black')
ES.set_xlabel('Time to Treatment')
ES.set_ylabel('Suicide per 1m Woman')
ES.axvline(-1, color = 'black', alpha=0.5)
ES.axhline(0, color = 'red')
plt.show()
```

In the figure we see, in general, relatively flat trends in the lead up to the event of interest, and thereafter a reduction in rates of female suicide following the passage of no fault divorce laws.  

### Interaction-weighted Estimator
To see how the interaction-weighted estimator proposed by @AbrahamSun2018 accounts for time-varying treatment adoption, we will generate it "by hand" here.   This estimator proposes to generate $\widehat{v}_g$ for some period $g$ of interest, where in this case $g$ will refer to each lag and lead. Formally, $\widehat{v}_g$ is defined as follows:
$$
  \widehat{v}_g = \frac{1}{|g|} \sum_{\ell \in g} \sum_{e} \widehat{\delta}_{e,\ell} \widehat{Pr} \left\{ E_i = e | E_i \in [-\ell , T - \ell] \right\}.
$$ {#eq-IWEstimator}
Here $E_i$ indicates the moment treatment is adopted for a unit $i$, and $\ell$ is the relative period to treatment at period $t$, ie $\ell = t - E_i$.  Thus, $\widehat{Pr} \left\{ E_i = e | E_i \in [-\ell , T - \ell] \right\}$ is the sample share of the cohort that receives the initial treatment at a time $e$, $g$ is a set of relative periods $\ell \in [-T , T]$, and $\widehat{\delta}_{e, \ell}$ is an estimate of the Cohort-specific Average Treatment effect on the Treated ($CATT$) for the cohort $e$ at $\ell$ periods from initial treatment $$\delta_{e,\ell} = CATT_{e, \ell} = E[Y_{i,e+\ell} - Y_{i,e+\ell}^{\infty} | E_i = e]$$ Where $Y_{i,t}$ is the outcome for unit $i$ at time $t$ and $Y_{i,t}^{\infty}$ is the potential outcome for unit $i$ at time $t$ if never were treated. @AbrahamSun2018 describe the estimation procedure of $\widehat{v}_g$ as follows:  

1. Estimate $CATT_{e,\ell}$ from a TWFE interacting relative periods indicators with cohort indicators, excluding indicators for cohorts from some set $C$^[In @AbrahamSun2018 you found a detailed explanation on how determinte the set $C$, for this example $C$ is the never treated units.]: $$Y_{i,t} = \alpha_i + \lambda_t + \sum_{e\neq C} \sum_{\ell \neq -1} \delta_{e, \ell} (\mathbf{1}\{ E_i = e\} \cdot D_{i,t}^{\ell}) + \varepsilon_{i,t}$$ Where $\alpha_i$ and $\lambda_t$ are the unit and time fixed effects, $\mathbf{1}\{E_i=e\}$ the cohort indicators and $D_{i,t}^{\ell}$ the relative period indicators, i.e, $D_{i,t}^{\ell} = 1$ if unit $i$ at time $t$ is $\ell$ periods from the treatment.  
2. Estimates the weights for each $\widehat{\delta}_{e,\ell}$: $\widehat{Pr} \left\{ E_i = e | E_i \in [-\ell , T - \ell] \right\}$ as the sample share of the cohort that receives the initial treatment at a time $e$ that has experienced the $\ell$ relative period to treatment.  
3. Estimate the IW estimator following equation (@eq-IWEstimator).

To fix ideas and get a hold on notation, in our example $g = \{-21 , -20 , \cdots , 27\}$, as we are considering the full set of lags and leads $\ell$ as part of $g$.  The years which a no fault divorce law was passed ($e$) are $e \in \{1969 , 1970 , 1971 , 1972 , 1973 , 1974 , 1975 , 1976 , 1977 , 1980 , 1984 , 1985\}$^[This can be seen by simply listing the set of adoption years, for example with: `data['_nfd'].unique()`.]. We will start by building a series of indicator variables for $E_i$ as follows:

```{python}
dummies_Cohort = pd.get_dummies(data['_nfd'].astype('Int64'), 
                                prefix = 'E', prefix_sep = '').astype(int)
data = pd.concat([data, dummies_Cohort], axis = 1) 
```
You may wish to confirm each `E1969` generated with the `get_dummies` function above contains a vector of 1s for all units which were first exposed to the policy in 1969, and so forth for other indicators.

If we return to (@eq-IWEstimator), we can see that we are interested in estimating a full set of lags and leads for each adoption period $e$.  With this particular setup, we have 12 indicator variables drawn from $e$, and if we consider all lags and leads in $g$, we have 48 indicator variables^[Excluding $\ell = -1$.]. Thus, from $\displaystyle\sum_{e\neq C}\sum_{\ell\neq-1}\delta_{e, \ell} (\mathbf{1}\{ E_i = e\} \cdot D_{i,t}^{\ell})$ we have 576 indicator variables! To ilustrate this we build three matrix: (i) Dummies for $g = \{-21 , -20 , \cdots , 27\}$, (ii) dummies for $e \in \{1969 , 1970 , 1971 , 1972 , 1973 , 1974 , 1975 , 1976 , 1977 , 1980 , 1984 , 1985\}$ and (iii) dummies for $CATT_{e,\ell}$.  At the end, we will also rename the column names of the last matrix in order to make it clearer when we go forward where we will store each of the outputs of interest.

```{python}
# Create the data frame for all interactions
for e in dummies_Cohort.columns:
    for ell in dummies_timeToTreat.drop(columns = 'Lead1').columns:
        if (e == "E1969") & (ell == 'Lead21'):
            dummies_CATT = pd.DataFrame(dummies_Cohort.loc[:,e] *
                                        dummies_timeToTreat.loc[:,ell])
        else:
            dummies_CATT = pd.concat([dummies_CATT,
                                      dummies_Cohort.loc[:,e] *
                                      dummies_timeToTreat.loc[:,ell]], axis = 1)

# Create colnames
colnames = []
for e in dummies_Cohort.columns:
    for ell in dummies_timeToTreat.drop(columns = 'Lead1').columns:
        colnames.append(str(e) + "_" + str(ell))

# Assign colnames
dummies_CATT.columns = colnames
```
The matrix `dummies_timeToTreat` will consist of an indicator for each observation capturing whether it is at a particular time to treatment adoption.  Similarly, `dummies_Cohort` will consists of an indicator for whether or not an observation is part of each group $e$.  The interaction between these two matrices (`dummies_CATT`) will thus build an indicator for each cohort and time to treatment, indicating whether an observation is in this particular group.  Note, however, that some of the indicator variables in `dummies_CATT` will actually be entirely empty. This is because there are some cohorts that never experience some specific $\ell$ relative to the period of treatment (eg early treatment adopters won't have enough data prior to treatment to observe very long leads, and late treatment adopters won't have enough post-treatment data to observe very long lags).  In order to delete these indicators which exist in our matrix `dummies_CATT` but not in practice, we can simply remove from the matrix `dummies_CATT` those columns with 0 mean:

```{python}
dummies_CATT = dummies_CATT.loc[:,dummies_CATT.mean() != 0]
len(dummies_CATT.columns)
```
As we see here, we have now reduced the dimensionality of the indicator variables $\displaystyle\sum_{e\neq C}\sum_{\ell\neq-1}\delta_{e, \ell} (\mathbf{1}\{ E_i = e\} \cdot D_{i,t}^{\ell})$ from 576 to 384, which are the full observable lags and leads for each treatment cohort. Now we can actually go about the business of estimating $\delta_{e,\ell}$!

```{python}
CATT_eldf = pd.concat([dummies_CATT, 
                       data[['pcinc', 'asmrh', 'cases', 'year', 'stfips',
                             'asmrs']]], axis=1)
CATT_el = Regpyhdfe(df = CATT_eldf, target = 'asmrs', 
                       predictors = list(CATT_eldf.loc[:,'E1969_Lead5':'cases'].columns),
                       absorb_ids=['stfips', 'year'],
                       cluster_ids=['stfips']).fit()
```

This looks quite simple, and it is precisely because we have gone to all the work of generating all the dummies we need for our CATT groups.  This, in essence, estimates an event study equivalent for each treatment adoption cohort.  As there is many estimates here, we don't show the full summary, but we can peruse the first 10 estimates for $CATT_{e,\ell}$:

```{python}
CATT_el.params[0:9]
```

To have a full idea of what we've just estimated here, we will re-organise these estimates to present the coefficient in the style of Table 3 of @AbrahamSun2018.  In particular, let's display $\ell$ values (lags and leads) in rows and $e$ values in columns so we can observe our cohort-specific event studies in a column-wise fashion.  We do this below, we first build a matrix `deltas` in which to store these estimates, then fill them in, before finally displaying the tabular output.  Most of this code is actually relatively auxiliary, used to ensure that we can extract each lag and lead from regression results.  To do this, we are generating a function we call `fetch_coefficient`, as the coefficient we need may sometimes be named with `_Lag` in the variable, sometimes be named as `_Lead`, and sometimes not exist (and this may imply either that it is the base period -1, or that the lag or lead doesn't exist).  It is worth working through this function carefully to confirm that you can see that in this way we grab each coefficient $\widehat\delta_{e,\ell}$. 

```{python}
# Matrix to store delta_{e,l}
deltas = pd.DataFrame(index = dummies_timeToTreat.columns,
                      columns = dummies_Cohort.columns)

# Function to fetch coefficient safely
import numpy as np
def fetch_coefficient(e, l):
    if l == -1:
        return 0
    elif l < -1:
        Catt_searched = 'E' + str(int(e)) + '_Lead' + str(abs(int(l)))
    else:
        Catt_searched = 'E' + str(int(e)) + '_Lag' + str(int(l))
    try:
        Value = CATT_el.params[Catt_searched]
    except:
        Value = np.nan
    return Value

# Get unique cohorts and relative times
cohorts = sorted(data['_nfd'].unique().astype(int))[1:]
ells = np.arange(-21, 28, 1)

# Fill the deltas matrix
for i in cohorts:
    print('----------------------------------------------------------')
    print('Cohort E = ' + str(i))
    for j in ells:
        if j < 0:
            print('Relative time ' + str(j) + '. This is Lead ' + str(abs(j)))
            deltas.loc['Lead' + str(abs(int(j))), 'E' + str(int(i))] = fetch_coefficient(e = i, l = j)
        else:
            print('Relative time ' + str(j) + '. This is Lag ' + str(j))
            deltas.loc['Lag' + str(int(j)), 'E' + str(int(i))] = fetch_coefficient(e = i, l = j)
        print('----')

# Display the coefficients in a nice tabular output
from IPython.display import Markdown
from tabulate import tabulate
Markdown(tabulate(deltas))
```

Now, with $\widehat\delta_{e,\ell}$ in hand, the only other thing we need are the weights of each cohort at the respective relative period.  We could do this "by hand", calculating from observations in our data, but it is likely easier to get these by regressing each cohort indicator variable $\mathbf{1} \{ E_i = e \}$ on all the relative period indicator variables $D^\ell_{i,t}$. This regression will just tell us the proportion of a specific lead or lag which are made up of observations from a particular cohort.  We will do this below, storing weights in a matrix called `w1`:

```{python}
# Matrix to store results
w1 = pd.DataFrame(index = dummies_timeToTreat.columns,
                  columns = dummies_Cohort.columns)
# Statsmodels for simple OLS
import statsmodels.api as sm

# For each cohort
for e in dummies_Cohort.columns:
    # Regress the cohort in indicated column on relative period dummies
    aux_model = sm.OLS(endog = dummies_Cohort.loc[:,e],
                       exog = dummies_timeToTreat.drop(columns = 'Lead1')).fit()
    # Assign the estimated coefficients that are the weights
    w1.loc['Lead21':'Lead2', e] =  aux_model.params['Lead21':'Lead2'].values
    w1.loc['Lag0':'Lag27', e] =  aux_model.params['Lag0':'Lag27'].values
    # Note those with 0 value really are relative periods for which the cohort doesn't
    # exist, so can be assignad as missing in order to follow Sun and Abraham Table 3
    w1.loc[:,e][w1.loc[:,e] == 0] = float('nan')
# Now the base period l = -1 replace for 0s
w1.loc['Lead1', :] = 0
w1.head()
```

<!--
Now we proceed with the second way that is to manually count the units in each cohort and identify cohorts present in each relative period. We start by get the amount of observations for each cohort at each relative time period to treatment in order to identify which cohorts were present in each relative time period to treatment. Then we continue by obtaining the number of units at each cohort of our sample

```{python}
# Identify cohorts present in each relative time period, if count > 0 the cohort
# is present
N_el = data.groupby(by = ['timeToTreat', '_nfd'], 
                    as_index = False).size().rename(columns = {'size': 'N_el'})
# Get the sample count of observations for each cohort
Ne = data.groupby(by = ['_nfd'], 
                  as_index = False).stfips.nunique().rename(columns = {'stfips': 'Ne'})
# Join this two datasets
Ns = pd.merge(left = N_el, right = Ne, how = 'left', 
              on = '_nfd').rename(columns = {'timeToTreat': 'l',
                                             '_nfd': 'e'}).drop(columns = 'N_el') 
Ns.head(5)
```


Now we build matrix for the amount of units at each cohort for each relative period to treatment in order to after compute the weights. This matrix is build in a similar way that `deltas` object

```{python}
# Matrix to store weights
units = pd.DataFrame(index = dummies_timeToTreat.columns,
                     columns = dummies_Cohort.columns)

# Function to fetch weights safely
def fetch_weight(e,l):
    if l == -1:
        return 0
    else:
        W_searched = Ns[(Ns['l'] == l) & (Ns['e'] == e)]
        if len(W_searched) == 0:
            return float('nan')
        else:
            return Ns.loc[W_searched.index, 'Ne'].values

# Fill the units matrix
for i in cohorts:
    print('----------------------------------------------------------')
    print('Cohort E = ' + str(i))
    for j in ells:
        if j < 0:
            print('Relative time ' + str(j) + '. This is Lead ' + str(abs(j)))
            units.loc['Lead' + str(abs(int(j))), 'E' + str(int(i))] = fetch_weight(e = i, l = j)
        else:
            print('Relative time ' + str(j) + '. This is Lag ' + str(j))
            units.loc['Lag' + str(int(j)), 'E' + str(int(i))] = fetch_weight(e = i, l = j)
        print('----')
# The matrix units is now filled
```
Now we build the weights for each $\widehat{\delta}_{e,\ell}$ as the share of cohort $e$ present in relative period $\ell$ and show that are (approximately) equal^[There may be some very small difference due to the method of estimation.] to the weights obtained in the regressions above

```{python}
w2 = pd.DataFrame(index = dummies_timeToTreat.columns,
                  columns = dummies_Cohort.columns)
rowSums = units.sum(axis = 1)
for e in dummies_Cohort.columns:
    for l in dummies_timeToTreat.drop(columns = 'Lead1').columns:
        if l == -1:
            w2.loc[l, e] = 0
        else:
            w2.loc[l, e] = units.loc[l,e] / rowSums[l]
w2.loc['Lead1', :] = 0
print(w1.iloc[0:9,0:9])
print(w2.iloc[0:9,0:9])
```
-->

We can display these weights in the same was as we documented the quantities $\widehat\delta_{e,\ell}$ previously:

```{python}
ws = w1
Markdown(tabulate(ws))
```

No, finally, we can generate $\widehat{v}_{\ell}$ for $\ell = -21, \ldots, 27$ and compare this with the results of the standard Panel Event Study Model we documented above.

```{python}
delta_l = (deltas * ws).sum(axis = 1)
coefs = pd.DataFrame(index = dummies_timeToTreat.columns,
                     columns = ['IW', 'ES'])
for l in dummies_timeToTreat.columns:
    coefs.loc[l, 'IW'] = delta_l[l]
    if l == 'Lead1':
        coefs.loc[l, 'ES'] = 0
    else:
        coefs.loc[l, 'ES'] = EventStudy.params[l]
Markdown(tabulate(coefs))
```

Scanning across coefficients, we can see that in this case, reassuringly, almost every coefficient at least keeps its sign, except for a few cases that weren't statiscally significant in the Panel Event Study Model.  

If we wanted to implement Sun and Abraham's estimator entirely by hand we could, and all that is missing to do this is to generate standard errors to build the confidence intervals. Following @AbrahamSun2018 we note that the variance of $\widehat{v}_\ell$ can be written as follows^[This comes from the followingproperty: Let $X,Y$ be random variables and $a,b\in\mathbb{R}$, then $V(aX \pm bY) = a^2V(X) + b^2V(Y) \pm 2abCov(X,Y)$.]
\begin{align*}
  V \left( \widehat{v}_{\ell} \right) & = V \left( \sum_{e \in h^\ell} \widehat{Pr} \{ E_i = e | E_i \in h^\ell \} \cdot \widehat{\delta}_{e,\ell} \right) \\
  & = \sum_{e \in h^\ell} \left( \widehat{Pr}\{ E_i = e | e \in h^\ell \} \right)^2 V \left( \widehat{\delta}_{e,\ell} \right) \\
  & + \sum_{e_j \in h^\ell} \sum_{e_k \in h^\ell , e_k \neq e_j} 2 \widehat{Pr}\{ E_i = e_j | e_j \in h^\ell \} \widehat{Pr}\{ E_i = e_k | e_k \in h^\ell \} Cov \left( \widehat{\delta}_{e_j,\ell} , \widehat{\delta}_{e_k,\ell} \right) \\
  & + \sum_{e \in h^\ell} \left( \widehat{\delta}_{e,\ell} \right)^2 V \left( \widehat{Pr}\{ E_i = e | e \in h^\ell \} \right) \\
  & + \sum_{e_j \in h^\ell} \sum_{e_k \in h^\ell , e_k \neq e_j} 2 \widehat{\delta}_{e_j,\ell}  \widehat{\delta}_{e_k,\ell} Cov \left( \widehat{Pr}\{ E_i = e_j | e_j \in h^\ell \} , \widehat{Pr}\{ E_i = e_k | e_k \in h^\ell \} \right)
\end{align*}
Where $h^\ell$ is the set of cohorts that experience the relative period $\ell$, variance and covariance of the $CATT_{e,\ell}$ come from estimates of step 1 and, variance and covariance of the weights $\widehat{Pr} \{ E_i = e | e \in h^\ell \}$ come from the regressions of $\mathbf{1} \{ E_i = e \}$ on $D^\ell_{i,t}$. 
** UNTIL HERE IS READY, BELOW HERE IS THE ERROR IN CODE, TEXT IS NOT MODIFIED YET NEITHER**
The key thing to see is that we could estimate this manually with the pieces we have already put together, and indeed, with the use of seemingly unrelated regression techniques it is possible to estimate the variance of $\widehat{Pr}(\cdot)$ and $\widehat\delta_{e\ell}$ in a single step.  However, in practice we will likely prefer to use a canned routine which allows for the estimation of the interaction weighted estimator, as well as the corresponding variance-covariance matrix. To see that our process of "manually" buidling up Sun and Abraham's estimator from its composite parts, and to additionally conduct inference in a direct way, we can use estimation routines such as `feols` and `sunab` functions from the `fixest` package in `R`. Below we do this, showing the point estimates recovered are identical to what we have done.  

Note the usage of the `sunab` function which expands dummies as we have done above first takes the variable that records treatment cohorts and next the variable which describes the relative time to treatment.  If the second variable points to the year, this function automatically and internally creates the relative time to treatment period. It is also important to see that we have slightly modify the data for those never treated units such that they take a very high value in the `X_nfd` variable, this is because the `sunab` function would drop this observations if the `X_nfd` has missing values (`NA`) in this variable and instead considers a never treated unit if the relative time to treatment is always negative. It is thus sufficient for a period beyond the last year of the data to be indicated, so that never treated are correctly viewed as units which have not *yet* adopted. Finally note the second part of the formula, after `|`, indicates the fixed effects and automatically clusters by the first variable:

```{python}
data = pd.read_csv("Datasets/StevensonWolfers2006.csv")
import paneleventstudy
print(data.dtypes)
data['stfips']=data['stfips'].astype(str)

paneleventstudy.balancepanel(data = data, group = 'stfips', event = 'post', 
                             calendartime = 'year')
data = paneleventstudy.identifycontrols(data = data, group = 'stfips', 
                                 event = 'post')
paneleventstudy.genreltime(data = data, group = 'stfips', event = 'post', 
                           calendartime = 'year', reltime = 'reltime', 
                           check_balance=True)
```


Finally, we plot the output along with its 95% confidence intervals to compare this with our original event study.

<!--
{r}
# Create Cols for IW Estimates Information
plot_df <- plot_df |> mutate(IWEstimate = NA,
                             IWSE = NA)
# Assign IW Estimates Information
plot_df[1:20, c("IWEstimate", "IWSE")] <- IW$coeftable[1:20, c("Estimate",
                                                               "Std. Error")]
plot_df[21, c("IWEstimate", "IWSE")] <- 0
plot_df[22:49, c("IWEstimate", "IWSE")] <- IW$coeftable[21:48, c("Estimate",
                                                               "Std. Error")]
# Graph
ggplot(data = plot_df) + geom_point(aes(x = Time, y = Estimate)) +
  geom_errorbar(aes(ymin = Estimate - 1.96 * SE, ymax = Estimate + 1.96 * SE, 
                    x = Time)) + 
  geom_point(aes(x = Time, y = IWEstimate), color = "blue",
             position = position_nudge(x = 0.4)) +
  geom_errorbar(aes(ymin = IWEstimate - 1.96 * IWSE, 
                    ymax = IWEstimate + 1.96 * IWSE, 
                    x = Time), color = "blue", 
                position = position_nudge(x = 0.4)) + 
  geom_hline(yintercept = 0, color = 'red') + geom_vline(xintercept = -1) +
  scale_x_continuous(limits = c(-21, 28), breaks = seq(-20, 25, 5)) +
  scale_y_continuous(limits = c(-40, 40), breaks = seq(-40, 40, 20)) +
  labs(x = 'Time To Treatment', y = 'Suicide per 1m Woman')

-->

## Code call-out 4.4: Synthetic control, difference-in-differences, and synthetic difference-in-differences

The synthetic control method seeks to construct a "synthetic control" for a treated unit (in this case, California) using a weighted combination of control units (other states). The aim is for this synthetic control to closely resemble the treated unit in the pre-treatment period based on predictor variables.

-   Use the 'synth' command to construct the synthetic control for California
-   Predictor variables: cigsale from specific years, beer, lnincome, retprice, age15to24
-   Treated unit: California (state==3)
-   Treatment period: 1989
-   Periods used to construct the synthetic control: 1980-1988

Once the synthetic control is constructed, we can compare the trends of the treated unit and the synthetic control in the post-treatment period. Any divergence in trends is interpreted as the treatment effect. In this case, we are assessing the impact of a hypothetical policy implemented in California in 1989 on cigarette sales.

```{python eval=FALSE}

# import pandas as pd
# from SyntheticControlMethods import Synth
# 
# # Load the Dataset
# df = pd.read_stata("Datasets/smoking.dta")
# 
# # Prepare the Data
# df = df.sort_values(by=['state', 'year'])
# 
# # Define the treatment period and the unit receiving the treatment
# treatment_period = 1989
# treated_unit = 'California'
# 
# # Predictor variables - Ensure these are present in your dataset
# predictors = ['cigsale', 'beer', 'lnincome', 'retprice', 'age15to24']
# 
# # Fit the Synthetic Control Model
# sc = Synth(df, "cigsale", "state", "year", treatment_period, treated_unit, pen=0)
# 
# # Visualize the Synthetic Control
# sc.plot(["original", "pointwise", "cumulative"], treated_label="California", 
#         synth_label="Synthetic California", treatment_label="Tobacco Policy Change")


```

**Results:**

-   The synthetic control for California is constructed using a combination of other states. Specifically, weights are assigned to states like Colorado, Connecticut, Montana, Nevada, New Mexico, and Utah.
-   The RMSPE (Root Mean Squared Prediction Error) is a measure of how well the synthetic control approximates California in the pre-treatment period. A lower RMSPE indicates a better fit. In this case, the RMSPE is 1.756235, suggesting a reasonably good fit.
-   The "Predictor Balance" table shows how California and the synthetic control compare in terms of the predictor variables. The figures show that there is a good balance between the treated unit and the synthetic control on these variables.
-   The graph displays per capita cigarette sales in California and the synthetic control over time. The divergence between the two lines post-1989 represents the estimated effect of the policy
